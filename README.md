# Ising FSS: äºŒç»´ Ising æ¨¡å‹æœ‰é™å°ºå¯¸æ ‡åº¦åˆ†æå·¥å…·åŒ…

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**Ising FSS** æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ã€é«˜æ€§èƒ½çš„äºŒç»´ Ising æ¨¡å‹æ¨¡æ‹Ÿä¸æœ‰é™å°ºå¯¸æ ‡åº¦ï¼ˆFinite-Size Scaling, FSSï¼‰åˆ†æå·¥å…·åŒ…ã€‚  
å®ƒé¢å‘ç»Ÿè®¡ç‰©ç†ã€å‡èšæ€ç‰©ç†ä»¥åŠæœºå™¨å­¦ä¹ æ–¹å‘çš„ç ”ç©¶è€…ä¸å­¦ç”Ÿï¼Œæä¾›ä» **è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ / å‰¯æœ¬äº¤æ¢ (REMC)** åˆ° **ä¸´ç•ŒæŒ‡æ•°æå– / æ•°æ®åç¼©** çš„å®Œæ•´å·¥ä½œæµã€‚

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ é«˜æ€§èƒ½æ¨¡æ‹Ÿå¼•æ“

- **å¤šç®—æ³•æ”¯æŒï¼ˆ2D Isingï¼Œå¶æ•°å°ºå¯¸ï¼Œå‘¨æœŸè¾¹ç•Œæ¡ä»¶ï¼‰**
  - Metropolisï¼ˆæ£‹ç›˜æ ¼åˆ†è§£å•è‡ªæ—‹ç¿»è½¬ï¼‰
  - Wolff å•ç°‡ç®—æ³•
  - Swendsenâ€“Wang å¤šç°‡ç®—æ³•  
  > ç°‡ç®—æ³•ä»…åœ¨é›¶å¤–åœº `h = 0` æ—¶å¯ç”¨ï¼›éé›¶å¤–åœºç”± Metropolis è´Ÿè´£ã€‚

- **CPU / GPU åŒåç«¯**
  - **CPU**ï¼šä½¿ç”¨ Numba/JIT åŠ é€Ÿçš„æ ¸å¿ƒæ›´æ–°ç®—å­ï¼Œé…åˆæ˜¾å¼ç§å­ç®¡ç†ï¼Œç»“æœå®Œå…¨å¯å¤ç°
  - **GPU**ï¼šåŸºäº CuPy çš„å¤§è§„æ¨¡å¹¶è¡Œå®ç°  
    - ä¸€æ¬¡å¯æ›´æ–°æ•°ç™¾è‡³ä¸Šåƒä¸ªå‰¯æœ¬  
    - ä½¿ç”¨ checkerboard åˆ†è§£é¿å…å†™å†²çª  
    - æ‰€æœ‰è‡ªæ—‹æ„å‹ä¸èƒ½é‡é©»ç•™åœ¨ GPU æ˜¾å­˜ä¸­ï¼Œåªåœ¨å¿…è¦æ—¶å›ä¼ 

- **å‰¯æœ¬äº¤æ¢è’™ç‰¹å¡æ´›ï¼ˆREMC / å¹³è¡Œå›ç«ï¼‰**
  - Slot-bound RNGï¼š**æ¸©åº¦æ§½** ä¸éšæœºæ•°æµä¸€ä¸€ç»‘å®šï¼Œå‰¯æœ¬äº¤æ¢æ—¶åªäº¤æ¢æ„å‹ï¼Œä¸äº¤æ¢ RNG çŠ¶æ€
  - æ˜¾å¼ `replica_seeds`ï¼šç”¨æˆ·å®Œå…¨æ§åˆ¶éšæœºæ€§ï¼ˆcheckpoint æ¢å¤æ—¶ä¸¥æ ¼æ ¡éªŒï¼‰
  - CPU / GPU ç‰ˆæœ¬å®ç°è¯­ä¹‰å¯¹é½ï¼š  
    - åˆå§‹åŒ–éšæœºæµä¸è¿è¡ŒæœŸéšæœºæµè§£è€¦ï¼ˆ`seed ^ const` æ´¾ç”Ÿåˆå§‹åŒ– RNGï¼‰
    - äº¤æ¢åˆ¤æ®ä¸èƒ½é‡å®šä¹‰åœ¨ CPU/GPU é—´ä¿æŒç»Ÿä¸€
  - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼š
    - CPUï¼šåŸºäº HDF5 çš„ checkpointï¼ˆåŒ…æ‹¬ latticeã€èƒ½é‡ã€RNG çŠ¶æ€ï¼‰
    - GPUï¼šJSON + NPZ åŒæ–‡ä»¶ç»“æ„ï¼ˆå…ƒæ•°æ® + ç‰©ç†æ€ï¼‰

---

### ğŸ“Š æœ‰é™å°ºå¯¸æ ‡åº¦åˆ†æ

- **è§‚æµ‹é‡åŸºç¡€ç®¡çº¿**
  - æ¯ä¸€æ¸©åº¦ `T` å¯¹åº”ä¸€æ¡æ—¶é—´åºåˆ—ï¼šèƒ½é‡å¯†åº¦ `E`ã€ç£åŒ–å¯†åº¦ `M`ã€`|M|`ã€`MÂ²`ã€`Mâ´`
  - `analyze()` è‡ªåŠ¨ç»™å‡ºï¼š
    - æ¯”çƒ­ï¼š`C`
    - ç£åŒ–ç‡ï¼š`Ï‡`
    - Binder ç´¯ç§¯é‡ï¼š`U`
    - æ ·æœ¬æ•°ï¼š`n_samples`

- **æœ‰é™å°ºå¯¸æ ‡åº¦ï¼ˆFSSï¼‰åˆ†æï¼ˆanalysis å­åŒ…ï¼‰**
  - æ”¯æŒä»¥ `{L, T, observable}` çš„ç»“æ„ç»„ç»‡æ•°æ®ï¼Œä¾¿äºä¹‹åï¼š
    - Binder äº¤å‰ç‚¹åˆ†æ (Tc ä¼°è®¡)
    - ä¸´ç•ŒæŒ‡æ•°æ‹Ÿåˆï¼ˆä¾‹å¦‚ Î½, Î³/Î½, Î²/Î½ï¼‰
    - æ•°æ®åç¼©ï¼ˆData Collapseï¼‰  
  - æ—¶é—´åºåˆ—ç»Ÿè®¡å·¥å…·ï¼š
    - è‡ªç›¸å…³æ—¶é—´ä¼°è®¡ï¼ˆSokal çª—å£æ³•ï¼‰
    - Moving-block Bootstrap è¯¯å·®ä¼°è®¡
    - é˜»å¡åˆ†æï¼ˆblockingï¼‰ä½œä¸ºå…œåº•æ–¹æ¡ˆ

> FSS çš„é«˜å±‚æ¥å£ï¼ˆå¦‚ `FSSAnalyzer`ï¼‰æ¨èåœ¨ `analysis/` æˆ– `examples/` ä¸­ç”¨è„šæœ¬æˆ– notebook å®ç°ï¼Œ
> ç›´æ¥å¯¹ `remc_simulator` / `gpu_remc_simulator` çš„ `analyze()` è¾“å‡ºè¿›è¡ŒäºŒæ¬¡å¤„ç†ã€‚

---

### ğŸ¤– æ·±åº¦å­¦ä¹ é›†æˆï¼ˆå¯é€‰ / æ‹“å±•æ–¹å‘ï¼‰

å·¥å…·åŒ…çš„æ¨¡æ‹Ÿç»“æœï¼ˆHDF5/NPZ æ ¼å¼ï¼‰é€‚åˆç›´æ¥ä½œä¸ºæ·±åº¦å­¦ä¹ æ•°æ®é›†ä½¿ç”¨ã€‚æ¨èåšæ³•ï¼š

- è‡ªè¡Œç¼–å†™ / ä½¿ç”¨ç¤ºä¾‹ä¸­çš„ PyTorch `Dataset` / `DataLoader`
  - æƒ°æ€§è¯»å– HDF5/NPZ
  - æŒ‰éœ€åš D4 ç¾¤æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ + ç¿»è½¬ï¼‰
  - æŒ‰ `T` æˆ–å…¶ä»–ç‰©ç†é‡æ‰“æ ‡ç­¾
- æ”¯æŒ Â±1 è‡ªæ—‹åˆ° [0,1] æˆ– [-1,1] çš„å¯é…ç½®æ˜ å°„ï¼Œä»¥é€‚é… VAE/CNN/Transformer ç­‰æ¨¡å‹

---

### ğŸ”¬ ç§‘å­¦è®¡ç®—æœ€ä½³å®è·µ

- **å®Œå…¨å¯å¤ç°**
  - æ‰€æœ‰éšæœºæ€§é€šè¿‡æ˜¾å¼ `replica_seeds` æ§åˆ¶
  - CPU/GPU ä¿æŒä¸€è‡´çš„ RNG ç­–ç•¥ï¼ˆPhilox ä¼˜å…ˆï¼Œå›é€€åˆ° `default_rng`ï¼‰
  - Checkpoint æ¢å¤æ—¶ä¼šä¸¥æ ¼æ£€æŸ¥ï¼š
    - ç³»ç»Ÿå°ºå¯¸ `L`
    - å¤–åœº `h`
    - æ¸©åº¦åˆ—è¡¨ `temps`
    - ç®—æ³•åç§° `algorithm`
    - `replica_seeds`ï¼ˆä¸åŒ¹é…æ—¶æ‹’ç»æ¢å¤ï¼‰

- **ç¨³å®šå¯é çš„ I/O**
  - GPU ä¾§ï¼šHDF5 æµå¼å†™å…¥ï¼›NPZ + JSON åŒæ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶ + `os.replace`ï¼Œé¿å…ä¸­é€”å´©æºƒäº§ç”ŸåŠæˆªæ–‡ä»¶ï¼‰
  - CPU ä¾§ï¼šHDF5 æµå¼å†™å…¥ + provenance è®°å½•ç»„ï¼ˆ`provenance`ï¼‰


---

## ğŸ“¦ å®‰è£…

### 1. åŸºç¡€ä¾èµ–ï¼ˆä»… CPUï¼‰

```bash
pip install numpy scipy h5py numba pyyaml
pip install -e .  # å¼€å‘æ¨¡å¼å®‰è£… ising-fss
```

### 2. GPU åŠ é€Ÿï¼ˆå¯é€‰ï¼‰

ç¡®ä¿ç³»ç»Ÿå·²æ­£ç¡®å®‰è£… CUDAï¼ˆæˆ– ROCm å¯¹åº”ç‰ˆæœ¬ï¼‰ã€‚

```bash
# æ ¹æ® CUDA ç‰ˆæœ¬é€‰æ‹©
pip install cupy-cuda11x
# æˆ–
pip install cupy-cuda12x
```

### 3. æ·±åº¦å­¦ä¹ æ‰©å±•ï¼ˆå¯é€‰ï¼‰

```bash
pip install torch torchvision       # PyTorch
pip install matplotlib seaborn      # åŸºç¡€å¯è§†åŒ–
# å¦‚éœ€è¦äº¤äº’å¼å›¾è¡¨ï¼š
# pip install plotly
```

---

## ğŸ¯ åº”ç”¨ç¤ºä¾‹

###  0. quick_startï¼ˆremc: metropolis_sweep ç®—æ³•ç¤ºä¾‹ï¼ŒCPUï¼‰

```python
# examples/quick_start.py
"""
Quick start: æœ€ç®€å•çš„ä¸€æ­¥ REMC ç¤ºä¾‹

- åœ¨ CPU ä¸Šç”¨ HybridREMCSimulator è·‘ä¸€ä¸ªå°ç³»ç»Ÿ (L=16, R=8)
- ä¸ä¾èµ– Config ç³»ç»Ÿï¼Œç›´æ¥ç”¨è£¸å‚æ•°
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds


def main():
    L = 16
    T_min, T_max = 1.0, 3.5
    num_replicas = 3

    # ç”Ÿæˆç¡®å®šæ€§çš„å‰¯æœ¬ç§å­
    replica_seeds = make_replica_seeds(master_seed=42, n_replicas=num_replicas)

    sim = HybridREMCSimulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        algorithm="metropolis_sweep",
        h=0.0,
        replica_seeds=replica_seeds,
    )

    sim.run(
        equilibration_steps=500,
        production_steps=2000,
        exchange_interval=5,
        thin=5,
        save_lattices=False,
        save_dir="runs/quick_start",
        worker_id="quick_start",
    )

    results = sim.analyze(verbose=False)

    # åªæ•°ä¸€æ•°æœ‰å¤šå°‘ä¸ªæ¸©åº¦æ¡ç›®ï¼ˆæ’é™¤ swap / seeds ç­‰å…¨å±€é¡¹ï¼‰
    T_keys = sorted(k for k in results.keys() if isinstance(k, str) and k.startswith("T_"))
    print(f"Got {len(T_keys)} temperature entries\n")

    # æ‰“å°æ¯ä¸ªæ¸©åº¦ç‚¹çš„ä¸»è¦è§‚æµ‹é‡
    print("Per-temperature observables:")
    for k in T_keys:
        v = results[k]
        T = float(k.replace("T_", ""))
        C = v["C"]; C_err = v["C_err"]
        chi = v["chi"]; chi_err = v["chi_err"]
        U = v["U"]
        n = v["n_samples"]
        print(
            f"{k} (T={T:.6f}): "
            f"C = {C:.4f} Â± {C_err:.4f}, "
            f"chi = {chi:.4f} Â± {chi_err:.4f}, "
            f"U = {U:.4f}, "
            f"n_samples = {n}"
        )

    # äº¤æ¢ç»Ÿè®¡ä¿¡æ¯
    swap = results.get("swap", {})
    print("\nSwap statistics:")
    print(f"  total attempts = {swap.get('attempt', 0)}")
    print(f"  total accepts  = {swap.get('accept', 0)}")
    print(f"  overall rate   = {swap.get('rate', 0.0):.4f}")
    pair_rates = swap.get("pair_rates", [])
    temps = swap.get("temps", [])
    for i, r in enumerate(pair_rates):
        if i + 1 < len(temps):
            print(
                f"  pair {i}: T={temps[i]:.4f} <-> T={temps[i+1]:.4f}, "
                f"accept rate = {r:.4f}"
            )

    # å¦‚æœæœ‰ warningï¼Œä¹Ÿæ‰“å°å‡ºæ¥çœ‹çœ‹
    if "warnings" in results:
        print("\nWarnings:")
        for w in results["warnings"]:
            print("  -", w)


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š
````
Got 3 temperature entries

Per-temperature observables:
T_1.000000 (T=1.000000): C = 4.8949 Â± 0.3306, chi = 61.5623 Â± 3.1173, U = 0.3386, n_samples = 400
T_1.870829 (T=1.870829): C = 0.8897 Â± 0.0622, chi = 3.4607 Â± 0.2231, U = 0.1404, n_samples = 400
T_3.500000 (T=3.500000): C = 0.3755 Â± 0.0126, chi = 49.2940 Â± 1.4780, U = 0.5535, n_samples = 400

Swap statistics:
  total attempts = 1000
  total accepts  = 985
  overall rate   = 0.9850
  pair 0: T=1.0000 <-> T=1.8708, accept rate = 0.9700
  pair 1: T=1.8708 <-> T=3.5000, accept rate = 1.0000
````


---

## ğŸ¯ Ising æ¨¡æ‹Ÿå‚æ•°é…ç½®æ–¹æ³•

### 1.1 åœ¨è„šæœ¬ä¸­ç›´æ¥æ„é€  Config

```python

# examples/inline_config_quick_start.py
"""
åœ¨è„šæœ¬ä¸­ç›´æ¥æ„é€  Configï¼Œç„¶åç”¨å…¶ä¸­çš„ simulation é…ç½®è·‘ä¸€æ¬¡ REMCã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.utils.config import SimulationConfig, DataConfig, Config, validate_config
from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds


def main():
    # ---- 1. æ„é€  Config ----
    sim_cfg = SimulationConfig(
        L=32,
        T_min=2.0,
        T_max=2.6,
        num_replicas=12,
        h_field=0.0,
        algorithm="metropolis",  # â†’ 'metropolis_sweep'
        boundary="pbc",
        backend="cpu",
        equilibration=2000,
        production=8000,
        exchange_interval=5,
        sampling_interval=5,
    )
    data_cfg = DataConfig(
        L=32,
        T_range=(2.0, 2.6),
        n_T=12,
        n_configs=2000,
        output_dir="data/config_inline_demo",
        export_pytorch=False,
    )
    cfg = Config(simulation=sim_cfg, data=data_cfg)

    has_warning, warning_list = validate_config(cfg)
    for w in warning_list:
        print("[config warning]", w)

    # ---- 2. æ„é€ æ¨¡æ‹Ÿå™¨ ----
    s = cfg.simulation
    replica_seeds = make_replica_seeds(master_seed=1234, n_replicas=s.num_replicas)

    sim = HybridREMCSimulator(
        L=s.L,
        T_min=s.T_min,
        T_max=s.T_max,
        num_replicas=s.num_replicas,
        algorithm=s.algorithm,
        h=s.h_field,
        replica_seeds=replica_seeds,
    )

    sim.run(
        equilibration_steps=s.equilibration,
        production_steps=s.production,
        exchange_interval=s.exchange_interval,
        thin=s.sampling_interval,
        save_lattices=True,
        save_dir=str(Path(data_cfg.output_dir) / "raw"),
        worker_id="inline_cfg",
    )

    print("Done. Raw REMC data written under", data_cfg.output_dir)


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š
````
Done. Raw REMC data written under data/config_inline_demo
````


### 1.2. CPU ä» YAML é…ç½®æ–‡ä»¶åŠ è½½ Config (REMC)

```python
# examples/run_from_yaml.py

import os
from ising_fss.utils.config import load_config, validate_config
from ising_fss.simulation.remc_simulator import HybridREMCSimulator
#  from ising_fss.simulation.gpu_remc_simulator import GPU_REMC_Simulator
from ising_fss.simulation.dispatcher import make_replica_seeds


def main():
    # 1. è¯»å– YAML é…ç½®å¹¶åšä¸€è‡´æ€§æ£€æŸ¥
    cfg = load_config("configs/config_L64.yaml")

    ok, warnings = validate_config(cfg)
    if not ok:
        for w in warnings:
            print("[config warning]", w)

    sim_cfg = cfg.simulation
    data_cfg = cfg.data

    # 2. æ ¹æ® backend é€‰æ‹© CPU / GPU ç‰ˆæœ¬çš„ REMC æ¨¡æ‹Ÿå™¨
    backend = sim_cfg.backend.lower()
    SimCls = GPU_REMC_Simulator if backend == "gpu" else HybridREMCSimulator

    # 3. ç”Ÿæˆæ˜¾å¼ replica_seedsï¼ˆHybrid/GPU ä¸¤ä¸ªç±»éƒ½è¦æ±‚æ˜¾å¼ç§å­ï¼‰
    master_seed = sim_cfg.seed or 0   # å¦‚æœ YAML é‡Œæ²¡å†™ seedï¼Œå°±ç”¨ 0 æˆ–ä½ å–œæ¬¢çš„æ•°
    replica_seeds = make_replica_seeds(master_seed, sim_cfg.num_replicas)

    # 4. æ„é€ æ¨¡æ‹Ÿå™¨å®ä¾‹
    sim = SimCls(
        L=sim_cfg.L,
        T_min=sim_cfg.T_min,
        T_max=sim_cfg.T_max,
        num_replicas=sim_cfg.num_replicas,
        algorithm=sim_cfg.algorithm,                  # å·²åœ¨ SimulationConfig é‡Œå½’ä¸€åŒ–
        spacing=getattr(sim_cfg, "temp_spacing", "geom"),
        h=sim_cfg.h_field,
        replica_seeds=replica_seeds,                  # â˜… å…³é”®ï¼šæ˜¾å¼ä¼ å…¥
    )

    # 5. è¿è¡Œ REMC
    outdir = "runs/L64_from_yaml"
    os.makedirs(outdir, exist_ok=True)

    thin = getattr(data_cfg, "sampling_interval", 1)  # é‡‡æ ·é—´éš”æ”¾åœ¨ DataConfig é‡Œ

    sim.run(
        equilibration_steps=sim_cfg.equilibration,
        production_steps=sim_cfg.production,
        exchange_interval=sim_cfg.exchange_interval,
        thin=thin,
        save_lattices=True,
        save_dir=outdir,
        worker_id=f"{backend}_yaml_demo",
    )

    stats = sim.analyze(verbose=True)
    print("å¹³å‡äº¤æ¢ç‡:", stats["swap"]["rate"])


if __name__ == "__main__":
    main()

```

````
# config_L64.yaml
# L = 64, åœ¨ä¸´ç•ŒåŒºåŸŸé™„è¿‘ç”¨ Metropolis + GPU åš REMCï¼Œ
# åŒæ—¶åœ¨æ›´å¤§åŒºé—´ [1.6, 3.2] ä¸Šç”Ÿæˆæœºå™¨å­¦ä¹ æ•°æ®é›†ã€‚

simulation:
  # æ™¶æ ¼å°ºå¯¸
  L: 64

  # è¿™é‡Œçš„ T_min / T_max ä¸»è¦æ˜¯â€œç‰©ç†å‚è€ƒçª—å£â€ï¼ˆä¸´ç•Œé™„è¿‘ï¼‰ï¼Œä¸ä¼šç›´æ¥ç”¨æ¥é“ºç‚¹ï¼›
  # çœŸå®çš„æ•°æ®ç½‘æ ¼ç”± data.T_range / data.n_T å†³å®šã€‚
  T_min: 2.20
  T_max: 2.35

  # REMC çš„å‰¯æœ¬æ•°ï¼ˆæ¸©åº¦æ•°ï¼‰ï¼›åœ¨ run_data_from_config.py çš„ REMC æ¨¡å¼ä¸‹ï¼Œ
  # ä¼šè¢« data.n_T è¦†ç›–ï¼ˆä»¥ data.T_range ä¸Šçš„æ¸©åº¦ç½‘æ ¼ä¸ºå‡†ï¼‰ã€‚
  num_replicas: 16

  # å¤–åœºï¼ˆæœ¬ç¤ºä¾‹åªåšé›¶å¤–åœºï¼‰
  h_field: 0.0

  # ç®—æ³•åç§°ï¼›ä¼šåœ¨ SimulationConfig ä¸­å½’ä¸€åŒ–ä¸º 'metropolis_sweep'
  algorithm: "metropolis"

  # è¾¹ç•Œæ¡ä»¶ï¼šå‘¨æœŸæ€§è¾¹ç•Œ (Periodic Boundary Conditions)
  boundary: "pbc"

  # åç«¯ï¼š'cpu' | 'gpu' | 'auto'
  # è¿™é‡Œä½¿ç”¨ GPUï¼Œè¦æ±‚ä½ å·²å®‰è£… CuPy ä¸” gpu_remc_simulator å¯ç”¨ã€‚
  # backend: "gpu"
  backend: "cpu"

  # æ¯æ¡é“¾çš„çƒ­åŒ–æ­¥æ•° / é‡‡æ ·æ­¥æ•°ä¸Šé™ï¼ˆä¸»è¦ç”¨äº GUI æˆ–å…¶å®ƒè„šæœ¬æ—¶åšå‚è€ƒï¼‰
  equilibration: 10000
  production: 20000

  # REMC äº¤æ¢é—´éš”ï¼ˆæ­¥æ•°ï¼‰
  exchange_interval: 10

  # éšæœºç§å­
  seed: 2025


data:
  # å¸Œæœ›æ•°æ®é›†ä¸­æ‰€æœ‰æ„å‹çš„ Lï¼›æ¨èä¸ simulation.L ä¿æŒä¸€è‡´
  L: 64

  # æ•°æ®é›†è¦è¦†ç›–çš„æ¸©åº¦èŒƒå›´ï¼ˆå…¨å±€ï¼‰
  T_range: [1.6, 3.2]

  # åœ¨ T_range ä¸Šå–å¤šå°‘ä¸ªæ¸©åº¦ç‚¹
  n_T: 40

  # æ¯ä¸ªæ¸©åº¦æœŸæœ›å¾—åˆ°å¤šå°‘ä¸ªæ ·æœ¬ï¼ˆå¤§è‡´å€¼ï¼‰
  n_configs: 1000

  # æ•°æ®ç”Ÿäº§ä¸“ç”¨çš„çƒ­åŒ–æ­¥æ•°ï¼ˆä¼˜å…ˆäº simulation.equilibrationï¼‰
  equilibration: 8192

  # é‡‡æ ·é—´éš”ï¼ˆthin çš„å€¼ï¼‰ï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ„å‹
  sampling_interval: 8

  # æ˜¯å¦åœ¨ä¸€ä¸ª REMC æ¨¡æ‹Ÿä¸­è·¨æ•´ä¸ª T_range
  #   true  â†’ REMC æ¨¡å¼ï¼šä¸€ä¸ªæ¨¡æ‹Ÿè¦†ç›–æ‰€æœ‰ tempsï¼ˆæ¸©åº¦ç½‘æ ¼ï¼‰
  #   false â†’ å•æ¸©åº¦æ¨¡å¼ï¼šæ¯ä¸ª T ç‹¬ç«‹è·‘ä¸€ä¸ª num_replicas=1 çš„ MC
  use_remc: true

  # å¤–åœºæ‰«æèŒƒå›´ï¼šæœ¬ç¤ºä¾‹ä¸åš h æ‰«æï¼Œå› æ­¤è®¾ä¸º null
  h_range: null

  # è¾“å‡ºç›®å½•ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œrun_data_from_config.py ä¼šç”¨å®ƒåˆ›å»º tmp/ã€merged/ã€pytorch/ï¼‰
  output_dir: "data/ising_L64"

  # æ˜¯å¦å¯¼å‡ºä¸º PyTorch è®­ç»ƒé›†
  export_pytorch: true

  # å¯¼å‡ºæ•°æ®ç±»å‹ï¼š'uint8' (ç´§å‡‘ï¼Œé€‚åˆå›¾åƒç±»ç½‘ç»œ) æˆ– 'float32'
  export_dtype: "uint8"

  # è®­ç»ƒ/éªŒè¯åˆ’åˆ†æ¯”ä¾‹
  train_split: 0.8

  # æ˜¯å¦å¯¹å¯¼å‡ºçš„æ•°æ®åšå½’ä¸€åŒ–ï¼ˆä¾‹å¦‚æŠŠè‡ªæ—‹æ˜ å°„åˆ° {0,1} æˆ– [0,1]ï¼‰
  normalize: true

````

è¾“å‡ºï¼š

````
[remc.analyze] sweep_index=30000 swap_rate=0.4615 total_attempts=45000 total_accepts=20769
å¹³å‡äº¤æ¢ç‡: 0.46153333333333335
````


---

### 1.3 ä½¿ç”¨ Config.from_args() + å‘½ä»¤è¡Œ --preset / --set / ENV æ¥é©±åŠ¨ REMCã€‚

```python
# examples/run_sim_with_from_args.py
"""
ä½¿ç”¨ Config.from_args() + å‘½ä»¤è¡Œ --preset / --set / ENV æ¥é©±åŠ¨ REMCã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.utils.config import from_args, validate_config
from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds


def main():
    cfg = from_args()  # ä¼šè§£æ --preset / --config / --set / ENV ç­‰
    has_warning, warning_list = validate_config(cfg)
    for w in warning_list:
        print("[config warning]", w)


    s = cfg.simulation
    replica_seeds = make_replica_seeds(master_seed=s.seed or 0, n_replicas=s.num_replicas)

    sim = HybridREMCSimulator(
        L=s.L,
        T_min=s.T_min,
        T_max=s.T_max,
        num_replicas=s.num_replicas,
        algorithm=s.algorithm,
        h=s.h_field,
        replica_seeds=replica_seeds,
    )

    sim.run(
        equilibration_steps=s.equilibration,
        production_steps=s.production,
        exchange_interval=s.exchange_interval,
        thin=s.sampling_interval,
        save_lattices=True,
        save_dir=str(Path(cfg.data.output_dir) / "raw_from_args"),
        worker_id="from_args",
    )

    print("Run finished. Output dir:", cfg.data.output_dir)


if __name__ == "__main__":
    main()

```
è°ƒç”¨ï¼š
```bash
python -m run_sim_with_from_args \
  --preset publication \
  --set simulation.L=64 \
  --set simulation.algorithm=metropolis \
  --set simulation.backend=cpu \
  --set data.output_dir="data/from_args_demo"
```

---

### 1.4 å°†æ¨¡æ‹Ÿæ•°æ®å¯¼å‡ºæˆ PyTorch å‹å¥½æ•°æ®é›†

```python
# examples/generate_dl_data.py
"""
ä» Config å‡ºå‘ï¼Œä¸€é”®ç”Ÿæˆç”¨äº DL çš„ HDF5 + PyTorch æ•°æ®é›†ã€‚

- ç¬¬ä¸€æ­¥ï¼šæ ¹æ® Config è·‘æ¨¡æ‹Ÿï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
- ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ ml.generate_dl_data.generate_from_hdf5 åšå¯¼å‡º
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.utils.config import from_args, validate_config
from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds
from examples.ml.generate_dl_data import generate_from_hdf5  # type: ignore


def main():
    #  è°ƒç”¨æ–¹æ³• 1.1 åœ¨è„šæœ¬ä¸­ç›´æ¥æ„é€  Config        
    #  sim_cfg = SimulationConfig(
    #      L=32,
    #      T_min=2.0,
    #      T_max=2.6,
    #      num_replicas=12,
    #      h_field=0.0,
    #      algorithm="metropolis",  # â†’ 'metropolis_sweep'
    #      boundary="pbc",
    #      backend="cpu",
    #      equilibration=2000,
    #      production=8000,
    #      exchange_interval=5,
    #      sampling_interval=5,
    #  )
    #  data_cfg = DataConfig(
    #      L=32,
    #      T_range=(2.0, 2.6),
    #      n_T=12,
    #      n_configs=2000,
    #      output_dir="data/config_inline_demo",
    #      export_pytorch=False,
    #  )
    #  cfg = Config(simulation=sim_cfg, data=data_cfg)
    #
    #  has_warning, warning_list = validate_config(cfg)
    #  for w in warning_list:
    #      print("[config warning]", w)
#
    cfg = from_args()
    warnings = validate_config(cfg)
    for w in warnings:
        print("[config warning]", w)

    s = cfg.simulation
    d = cfg.data
    out_root = Path(d.output_dir)
    raw_dir = out_root / "raw"

    # åªåšä¸€ä¸ªç®€å•é€»è¾‘ï¼šå¦‚æœ raw_dir ä¸‹æ²¡æœ‰ä»»ä½• .h5ï¼Œå°±è·‘ä¸€æ¬¡ REMC
    if not any(raw_dir.glob("*.h5")):
        raw_dir.mkdir(parents=True, exist_ok=True)
        replica_seeds = make_replica_seeds(master_seed=s.seed or 0, n_replicas=s.num_replicas)
        sim = HybridREMCSimulator(
            L=s.L,
            T_min=s.T_min,
            T_max=s.T_max,
            num_replicas=s.num_replicas,
            algorithm=s.algorithm,
            h=s.h_field,
            replica_seeds=replica_seeds,
        )
        sim.run(
            equilibration_steps=s.equilibration,
            production_steps=s.production,
            exchange_interval=s.exchange_interval,
            thin=s.sampling_interval,
            save_lattices=True,
            save_dir=str(raw_dir),
            worker_id="dl_from_config",
        )

    # è°ƒç”¨ ML ç«¯å¯¼å‡º
    generate_from_hdf5(
        raw_dir=raw_dir,
        out_dir=out_root / "pytorch",
        normalize=True,
        dtype="uint8",
    )


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ä» Config å‡ºå‘ï¼Œä¸€é”®ç”Ÿæˆç”¨äº DL çš„ HDF5 + PyTorch æ•°æ®é›†ã€‚

- ç¬¬ä¸€æ­¥ï¼šæ ¹æ® Config è·‘ REMCï¼ˆå¦‚æœ raw_dir é‡Œè¿˜æ²¡æœ‰ .h5ï¼‰
- ç¬¬äºŒæ­¥ï¼šç›´æ¥åœ¨æœ¬æ–‡ä»¶ä¸­ï¼Œä» HDF5 è¯»å‡º configsï¼Œå¹¶å¯¼å‡ºä¸º PyTorch å‹å¥½çš„å¸ƒå±€
"""

from __future__ import annotations

import sys
import logging
from pathlib import Path
from typing import Union

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.utils.config import from_args, validate_config
from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds
from ising_fss.data.config_io import load_configs_hdf5, export_for_pytorch

logger = logging.getLogger("generate_dl_data")
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

PathLike = Union[str, Path]


def _flatten_configs(configs: np.ndarray) -> np.ndarray:
    """
    å°† HDF5 é‡Œè¯»å‡ºçš„ configs ç»Ÿä¸€æˆ (N, L, L)ã€‚

    æ”¯æŒä¸¤ç§å…¸å‹å¸ƒå±€ï¼š
        - (N, L, L)
        - (n_h, n_T, n_c, L, L)  -> å±•å¹³æˆ (N, L, L)
    """
    arr = np.asarray(configs)
    if arr.ndim == 3:
        return arr
    if arr.ndim == 5:
        n_h, n_T, n_c, Lx, Ly = arr.shape
        return arr.reshape(n_h * n_T * n_c, Lx, Ly)
    raise ValueError(f"Unexpected configs ndim={arr.ndim}, expected 3 or 5.")


def _export_pytorch_from_hdf5(
    raw_dir: PathLike,
    out_dir: PathLike,
    *,
    normalize: bool = True,
    dtype: str = "uint8",
    split_ratio: float = 0.8,
    seed: int = 0,
) -> None:
    """
    ä» REMC ç”Ÿæˆçš„ HDF5 åŸå§‹æ™¶æ ¼æ–‡ä»¶ä¸­ï¼Œæ„é€ ä¸€ä¸ª PyTorch å‹å¥½çš„æ•°æ®é›†ã€‚

    raw_dir ä¸‹åº”å½“æœ‰è‹¥å¹² .h5 æ–‡ä»¶ï¼ˆç”± HybridREMCSimulator ä¿å­˜ï¼‰ã€‚
    """
    raw_dir = Path(raw_dir)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    h5_files = sorted(raw_dir.glob("*.h5"))
    if not h5_files:
        raise FileNotFoundError(f"No .h5 files found under {raw_dir}")

    logger.info("Found %d HDF5 files under %s", len(h5_files), raw_dir)

    exported = False
    for h5 in h5_files:
        logger.info("Try loading configs from %s", h5)
        try:
            ds_raw = load_configs_hdf5(h5, load_configs=True, load_obs=True)
        except Exception as exc:
            logger.warning("load_configs_hdf5 failed for %s: %s", h5, exc)
            continue

        if "configs" not in ds_raw:
            logger.warning("No 'configs' field in %s; skip.", h5)
            continue

        configs = _flatten_configs(np.asarray(ds_raw["configs"]))
        if configs.ndim != 3:
            logger.warning("Unexpected configs ndim=%d in %s; skip.", configs.ndim, h5)
            continue

        N, Lx, Ly = configs.shape
        if Lx != Ly:
            logger.warning("Non-square lattice (%d x %d) in %s; skip.", Lx, Ly, h5)
            continue

        logger.info("Configs shape: N=%d, L=%d", N, Lx)

        # ä¸ºäº†ç®€å•/ç¨³å¥ï¼Œæ ‡ç­¾å’Œè§‚æµ‹é‡å…ˆå…¨ 0 å ä½ï¼Œå®Œå…¨ç”±ä¸‹æ¸¸ä»»åŠ¡è‡ªç”±ä½¿ç”¨
        temps = np.zeros(N, dtype=np.float32)
        fields = np.zeros(N, dtype=np.float32)
        energy = np.zeros(N, dtype=np.float32)
        magnetization = np.zeros(N, dtype=np.float32)

        ds_pt = {
            "configs": configs,
            "temperatures": temps,
            "fields": fields,
            "energy": energy,
            "magnetization": magnetization,
            "parameters": {
                "L": int(Lx),
                "n_configs": int(N),
                "generator": "config.generate_dl_data",
                "source_file": str(h5),
            },
        }

        logger.info(
            "Exporting PyTorch dataset to %s (normalize=%s, dtype=%s, split_ratio=%.3f, seed=%d)...",
            out_dir,
            normalize,
            dtype,
            split_ratio,
            seed,
        )

        export_for_pytorch(
            ds_pt,
            out_dir,
            split_ratio=split_ratio,
            normalize=normalize,
            dtype=dtype,
            seed=seed,
        )

        exported = True
        logger.info("PyTorch export succeeded from %s", h5)
        break

    if not exported:
        raise RuntimeError(
            f"Failed to export PyTorch dataset: no suitable HDF5 file "
            f"with 'configs' found under {raw_dir}"
        )


def main():
    #  è°ƒç”¨æ–¹æ³• 1.1 åœ¨è„šæœ¬ä¸­ç›´æ¥æ„é€  Config        
    #  sim_cfg = SimulationConfig(
    #      L=32,
    #      T_min=2.0,
    #      T_max=2.6,
    #      num_replicas=12,
    #      h_field=0.0,
    #      algorithm="metropolis",  # â†’ 'metropolis_sweep'
    #      boundary="pbc",
    #      backend="cpu",
    #      equilibration=2000,
    #      production=8000,
    #      exchange_interval=5,
    #      sampling_interval=5,
    #  )
    #  data_cfg = DataConfig(
    #      L=32,
    #      T_range=(2.0, 2.6),
    #      n_T=12,
    #      n_configs=2000,
    #      output_dir="data/config_inline_demo",
    #      export_pytorch=False,
    #  )
    #  cfg = Config(simulation=sim_cfg, data=data_cfg)
    #
    #  has_warning, warning_list = validate_config(cfg)
    #  for w in warning_list:
    #      print("[config warning]", w)
#


    # 1.2 ä»å‘½ä»¤è¡Œ / YAML è¯»å– Config
    cfg = from_args()

    has_problem, warning_list = validate_config(cfg)
    for w in warning_list:
        print("[config warning]", w)

    s = cfg.simulation
    d = cfg.data

    out_root = Path(d.output_dir)
    raw_dir = out_root / "raw"
    pt_dir = out_root / "pytorch"

    #  å¦‚æœ raw_dir ä¸‹æ²¡æœ‰ .h5ï¼Œå°±è·‘ä¸€æ¬¡ REMC
    if not any(raw_dir.glob("*.h5")):
        raw_dir.mkdir(parents=True, exist_ok=True)
        replica_seeds = make_replica_seeds(
            master_seed=s.seed or 0,
            n_replicas=s.num_replicas,
        )

        logger.info(
            "Running REMC: L=%d, Tâˆˆ[%.3f, %.3f], replicas=%d, eq=%d, prod=%d, thin=%d",
            s.L, s.T_min, s.T_max, s.num_replicas, s.equilibration, s.production, s.sampling_interval
        )

        sim = HybridREMCSimulator(
            L=s.L,
            T_min=s.T_min,
            T_max=s.T_max,
            num_replicas=s.num_replicas,
            algorithm=s.algorithm,
            h=s.h_field,
            replica_seeds=replica_seeds,
        )

        sim.run(
            equilibration_steps=s.equilibration,
            production_steps=s.production,
            exchange_interval=s.exchange_interval,
            thin=s.sampling_interval,
            save_lattices=True,
            save_dir=str(raw_dir),
            worker_id="dl_from_config",
        )
        logger.info("REMC finished. Raw HDF5 saved under %s", raw_dir)
    else:
        logger.info("Found existing .h5 files under %s, skip REMC simulation.", raw_dir)

    #   ä» HDF5 å¯¼å‡º PyTorch æ•°æ®
    #   å°è¯•ä» DataConfig é‡Œè¯»å‡ºä¸€äº›å‚æ•°ï¼Œä¸å­˜åœ¨å°±ç”¨é»˜è®¤å€¼
    normalize = getattr(d, "normalize", True)
    dtype = getattr(d, "dtype", "uint8")
    split_ratio = getattr(d, "split_ratio", 0.8)
    seed = getattr(s, "seed", 0) or 0

    _export_pytorch_from_hdf5(
        raw_dir=raw_dir,
        out_dir=pt_dir,
        normalize=normalize,
        dtype=dtype,
        split_ratio=split_ratio,
        seed=seed,
    )

    print("Done. Raw REMC data in", raw_dir)
    print("      PyTorch-ready dataset in", pt_dir)


if __name__ == "__main__":
    main()



```

 1.2 ä»å‘½ä»¤è¡Œ / YAML è¯»å– Config
```bash
python generate_dl_data.py --config configs/config_L64.yaml     
```

1.3 ä½¿ç”¨ Config.from_args() + å‘½ä»¤è¡Œ --preset / --set / ENV æ¥é©±åŠ¨ REMCã€‚
```bash
python -m run_sim_with_from_args \
  --preset publication \
  --set simulation.L=64 \
  --set simulation.algorithm=metropolis \
  --set simulation.backend=cpu \
  --set data.output_dir="data/from_args_demo"
```

## æ¨¡æ‹Ÿæ–¹æ³•é€‰æ‹©

### 2.1  CPU æ¨¡å¼ä¸‹çš„ REMC æ¨¡æ‹Ÿ

```python
# examples/cpu_remc_basic.py
"""
CPU REMC åŸºæœ¬ç¤ºä¾‹ï¼šHybridREMCSimulator + make_replica_seeds
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds


def main():
    L = 16
    T_min, T_max = 2.0, 2.6
    num_replicas = 8

    replica_seeds = make_replica_seeds(master_seed=2024, n_replicas=num_replicas)
    sim = HybridREMCSimulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        algorithm="metropolis_sweep",
        h=0.0,
        replica_seeds=replica_seeds,
    )

    sim.run(
        equilibration_steps=1000,
        production_steps=5000,
        exchange_interval=5,
        thin=5,
        save_lattices=False,
        save_dir="runs/cpu_basic",
        worker_id="cpu_basic",
    )
    stats = sim.analyze(verbose=False)
    print("Finished CPU REMC. #temps =", len(stats))


if __name__ == "__main__":
    main()

```

è¾“å‡ºï¼š
````
Finished CPU REMC. #temps = 13
````

### 2.2 GPU æ¨¡å¼ä¸‹çš„ REMC

```python
# examples/gpu_remc_basic.py
"""
GPU REMC åŸºæœ¬ç¤ºä¾‹ï¼šGPU_REMC_Simulator + äº¤æ¢ç‡ / è€—æ—¶
"""

from __future__ import annotations

import sys
import time
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.dispatcher import make_replica_seeds, gpu_available

try:
    from ising_fss.simulation.gpu_remc_simulator import GPU_REMC_Simulator
except ImportError:
    GPU_REMC_Simulator = None  # type: ignore


def main():
    if not gpu_available() or GPU_REMC_Simulator is None:
        print("âŒ GPU/CuPy ä¸å¯ç”¨ï¼Œæœ¬ç¤ºä¾‹æ— æ³•è¿è¡Œã€‚")
        return

    L = 64
    T_min, T_max = 2.0, 2.6
    num_replicas = 32

    replica_seeds = make_replica_seeds(master_seed=2025, n_replicas=num_replicas)
    sim = GPU_REMC_Simulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        algorithm="metropolis",
        h=0.0,
        replica_seeds=replica_seeds,
    )

    t0 = time.time()
    sim.run(
        equilibration_steps=2000,
        production_steps=10000,
        exchange_interval=5,
        thin=20,
        save_lattices=False,
        save_dir="runs/gpu_basic",
        worker_id="gpu_basic",
    )
    dt = time.time() - t0
    res = sim.analyze(verbose=False)
    swap = res.get("swap", {})
    print(f"Finished GPU REMC in {dt:.2f}s, swap rate â‰ˆ {swap.get('rate', 'N/A')}")


if __name__ == "__main__":
    main()

```

### 2.3 parallel å¹¶è¡Œæ¨¡å¼ä¸‹çš„ Ising æ¨¡æ‹Ÿ

```python
# examples/parallel_across_L.py
"""
parallel.across_Lï¼šå¤š L å¹¶è¡Œ + checkpoint æ¢å¤ç¤ºä¾‹
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.parallel import across_L


def main():
    L_list = [16, 32, 64]
    out_ckpt = Path("runs/parallel_ckpt")
    out_ckpt.mkdir(parents=True, exist_ok=True)

    results = across_L(
        L_list=L_list,
        T_min=2.0,
        T_max=2.6,
        num_replicas=32,
        equilibration=2000,
        production=5000,
        algorithm="wolff",
        exchange_interval=5,
        thin=5,
        n_processes_per_L=1,
        checkpoint_dir=str(out_ckpt),
        checkpoint_final=True,
        resume_if_exists=True,
    )

    print("\nSummary:")
    for L, res in results.items():
        if isinstance(res, dict) and "error" in res:
            print(f" L={L}: ERROR -> {res['error']}")
        else:
            swap = res.get("swap", {})
            print(f" L={L}: swap rate â‰ˆ {swap.get('rate', 'N/A')}")


if __name__ == "__main__":
    main()

```

è¾“å‡ºï¼š
````
[worker pid=33062] Starting L=16  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=33060] Starting L=32  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=33061] Starting L=64  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=33062] L=16 å·²ä¿å­˜ checkpoint -> remc_L16_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=33062] L=16 completed
[worker pid=33060] L=32 å·²ä¿å­˜ checkpoint -> remc_L32_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=33060] L=32 completed
[worker pid=33061] L=64 å·²ä¿å­˜ checkpoint -> remc_L64_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=33061] L=64 completed

Summary:
 L=16: swap rate â‰ˆ 0.9556912442396314
 L=32: swap rate â‰ˆ 0.9604377880184332
 L=64: swap rate â‰ˆ 0.9708755760368664
````

### 2.4 batch å¹¶è¡Œæ¨¡å¼ä¸‹çš„ REMC
```python
# examples/batch_worker_remc.py
"""
ç›´æ¥åœ¨ Python è„šæœ¬ä¸­è°ƒç”¨ batch_runner.main(argv) å¯åŠ¨å¤š worker REMCã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation import batch_runner


def main():
    outdir = Path("runs/batch_worker_demo")
    outdir.mkdir(parents=True, exist_ok=True)

    argv = [
        "--mode", "run_workers",
        "--outdir", str(outdir),
        "--nworkers", "2",
        "--L", "32",
        "--T", "2.269",
        "--equil", "2000",
        "--prod", "5000",
        "--exchange_interval", "5",
        "--thin", "10",
        "--replicas", "16",
        "--algo", "metropolis_sweep",
        "--spacing", "geom",
        "--h", "0.0",
        "--save_lattices",
    ]
    batch_runner.main(argv)
    print("Workers finished. You can now run merge via 05_batch_demo_cli.py or CLI.")


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š
````
[worker 1] save_dir -> runs/batch_worker_demo/tmp/worker_pid33089_1cab7f10_w1
[worker 0] save_dir -> runs/batch_worker_demo/tmp/worker_pid33088_09d2d812_w0
[worker 1] sim.run completed -> runs/batch_worker_demo/tmp/worker_pid33089_1cab7f10_w1
[worker 0] sim.run completed -> runs/batch_worker_demo/tmp/worker_pid33088_09d2d812_w0
All worker processes finished. You can now run --mode merge to combine results.
Workers finished. You can now run merge via 05_batch_demo_cli.py or CLI.
````

### 2.4.1 batch æ¨¡å¼çš„å¦ä¸€ç§å¯åŠ¨æ–¹å¼
```python
# examples/batch_demo_cli.py
"""
å±•ç¤ºå‡ æ¡æ¨èçš„ batch_runner å‘½ä»¤è¡Œã€‚

æœ¬æ–‡ä»¶ä¸ç›´æ¥è·‘ï¼Œåªæ˜¯ç»™ç”¨æˆ· copy ç²˜è´´ç”¨ã€‚
"""

EXAMPLE_RUN = r"""
# å¯åŠ¨ 4 ä¸ª workerï¼Œåœ¨ L=64ã€T=2.269 é™„è¿‘è¿›è¡Œ REMC é‡‡æ ·
python -m ising_fss.simulation.batch_runner \
  --mode run_workers \
  --outdir data/ising_L64_batch \
  --nworkers 4 \
  --L 64 \
  --T 2.269 \
  --equil 5000 \
  --prod 20000 \
  --exchange_interval 10 \
  --thin 10 \
  --replicas 32 \
  --algo metropolis_sweep \
  --spacing geom \
  --h 0.0 \
  --save_lattices
"""

EXAMPLE_MERGE = r"""
# åœ¨åŒä¸€ä¸ª outdir ä¸‹è¿›è¡Œåˆå¹¶
python -m ising_fss.simulation.batch_runner \
  --mode merge \
  --outdir data/ising_L64_batch
"""

if __name__ == "__main__":
    print("==== batch_runner run_workers ç¤ºä¾‹ ====")
    print(EXAMPLE_RUN)
    print("\n==== batch_runner merge ç¤ºä¾‹ ====")
    print(EXAMPLE_MERGE)

```

### 2.5 dispatcher æ¨¡å¼ä¸‹çš„å• REMC

```python
# examples/dispatcher_single_replica.py
"""
dispatcher.apply_move: å•å‰¯æœ¬ä¸€æ­¥æ›´æ–°ç¤ºä¾‹
"""

from __future__ import annotations

import sys
from pathlib import Path

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation import dispatcher


def main():
    L = 16
    beta = 1.0 / 2.269
    spins = np.random.choice([-1, 1], size=(L, L)).astype(np.int8)

    new_spins, info = dispatcher.apply_move(
        spins,
        beta,
        replica_seed=123,
        algo="metropolis_sweep",
        backend="auto",
    )

    print("Single replica update done.")
    print("Accepted moves:", info.get("accepted", "N/A"))


if __name__ == "__main__":
    main()

```

è¾“å‡ºï¼š

````
Single replica update done.
Accepted moves: 149
````

### 2.6 dispatcher æ¨¡å¼ä¸‹çš„å¤š REMC

```python
# examples/dispatcher_multi_replicas.py
"""
dispatcher.apply_move_batch: å¤šå‰¯æœ¬æ‰¹é‡æ›´æ–°ç¤ºä¾‹
"""

from __future__ import annotations

import sys
from pathlib import Path

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation import dispatcher


def main():
    R, L = 8, 16
    betas = [1.0 / 2.269] * R
    spins_batch = np.random.choice([-1, 1], size=(R, L, L)).astype(np.int8)

    replica_seeds = dispatcher.make_replica_seeds(master_seed=999, n_replicas=R)

    new_batch, meta = dispatcher.apply_move_batch(
        spins_batch,
        betas,
        replica_seeds=replica_seeds,
        algo="metropolis_sweep",
        backend="cpu",
        n_sweeps=10,
    )

    print("Batch update done.")
    print("meta keys:", meta.keys())


if __name__ == "__main__":
    main()

```

è¾“å‡ºï¼š
````
Batch update done.
meta keys: dict_keys(['per_replica'])
````

---

## åˆ†æ

### 3.1 ä» REMC è¾“å‡ºç›®å½• / HDF5 / NPZ åŠ è½½æ•°æ®ï¼Œåš E/M åŠ FSS ç»Ÿè®¡é‡çš„ä½œå›¾ã€‚
```python
# examples/load_and_analyze.py
"""
ä» REMC è¾“å‡ºç›®å½• / HDF5 / NPZ åŠ è½½æ•°æ®ï¼Œåš E/M åŠ FSS ç»Ÿè®¡é‡çš„ä½œå›¾ã€‚

ä¸»è¦ä½¿ç”¨åœºæ™¯ï¼š
  python load_and_analyze.py /path/to/remc_output_dir

å…¶ä¸­ remc_simulator / GPU_REMC_Simulator çš„è¾“å‡ºç›®å½•é‡ŒåŒ…å«ï¼š
  - è‹¥å¹² HDF5 æ ¼å¼çš„æ ¼ç‚¹æ–‡ä»¶ï¼š
        <worker_prefix>__latt_T_2.350000_h0.000000.h5
        <worker_prefix>__latt_T_2.400000_h0.000000.h5
        ...
  - å¯¹åº”çš„å…ƒæ•°æ® JSONï¼š
        <worker_prefix>__metadata.json

æœ¬è„šæœ¬ä¼šï¼š
  1. æ‰«æç›®å½•ä¸­æ‰€æœ‰ HDF5ï¼ŒæŒ‰ worker_prefix åˆ†ç»„ï¼›
  2. å¯¹æ¯ä¸ª workerï¼š
     - ä»æ‰€æœ‰ HDF5 ä¸­æå– E/M åºåˆ—ï¼Œè®¡ç®— <E>(T)ã€<m>(T)ã€C(T)ã€Ï‡(T)ã€U(T)ï¼Œå¹¶ä½œå›¾ï¼›
     - è‹¥å­˜åœ¨ worker_prefix__metadata.jsonï¼Œä¸”å…¶ä¸­åŒ…å« thermo_stats/swap ä¿¡æ¯ï¼Œ
       åˆ™å†åšä¸€å¼ å¸¦è¯¯å·®æ¡çš„ C/Ï‡/U å›¾ã€ä»¥åŠäº¤æ¢ç‡ç»Ÿè®¡å›¾ã€‚
"""

from __future__ import annotations

import sys
import re
import json
from collections import defaultdict
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import matplotlib.pyplot as plt

# è®© examples/* èƒ½æ‰¾åˆ°é¡¹ç›®é‡Œçš„ src/
ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.data.config_io import load_configs_hdf5
from ising_fss.core.observables import _energy_total_numpy as energy_fn


# ------------------------------------------------------------------
# ä¸€äº›å°å·¥å…·
# ------------------------------------------------------------------
def _parse_worker_T_h_from_name(name: str) -> Optional[Tuple[str, float, float]]:
    """
    ä»æ–‡ä»¶åä¸­è§£æ worker å‰ç¼€ / T / h

    æœŸæœ›æ ¼å¼ç±»ä¼¼ï¼š
        cpu_yaml_demo__latt_T_2.350000_h0.000000.h5

    è¿”å›:
        (worker_prefix, T, h) æˆ– None
    """
    m = re.match(r"(.+?)__latt_T_([-0-9.]+)_h([-0-9.]+)\.h5$", name)
    if not m:
        return None
    worker = m.group(1)
    T = float(m.group(2))
    h = float(m.group(3))
    return worker, T, h


def _compute_observables_from_configs(configs: np.ndarray,
                                      T: float,
                                      h: float) -> dict:
    """
    ç»™å®šæŸä¸ªæ¸©åº¦ä¸‹çš„å…¨éƒ¨æ„å‹ (N, L, L)ã€æ¸©åº¦ Tã€å¤–åœº hï¼Œ
    è®¡ç®— E(t)ã€M(t)ã€ä»¥åŠ C(T)ã€chi(T)ã€Binder U(T)ã€‚

    è¿”å› dict:
        {
            "T": T,
            "h": h,
            "E_series": E_per_spin_array,  # shape (N,)
            "M_series": M_per_spin_array,  # shape (N,)
            "E_mean": ...,
            "M_mean": ...,
            "C": ...,
            "chi": ...,
            "U": ...,
            "n_samples": N,
        }
    """
    configs = np.asarray(configs)
    assert configs.ndim == 3, f"configs must be (N,L,L), got {configs.shape}"
    N_samples, L, _ = configs.shape
    N_site = L * L
    beta = 1.0 / float(T)

    E = np.empty(N_samples, dtype=np.float64)
    M = np.empty(N_samples, dtype=np.float64)

    for i, cfg in enumerate(configs):
        spins = np.asarray(cfg, dtype=np.int8)
        # æ€»èƒ½é‡
        e_tot = energy_fn(spins, h=h)
        # æ¯è‡ªæ—‹èƒ½é‡ / ç£åŒ–
        E[i] = e_tot / N_site
        M[i] = spins.mean()

    # ä¸€é˜¶ç»Ÿè®¡
    E_mean = float(np.mean(E))
    M_mean = float(np.mean(M))

    # æ¯”çƒ­ C(T) å’Œç£åŒ–ç‡ Ï‡(T)ï¼ˆç®€å•æ–¹å·®ï¼Œä¸è€ƒè™‘è‡ªç›¸å…³ä¿®æ­£ï¼‰
    var_E = float(np.var(E))
    var_M = float(np.var(M))

    C = beta * beta * N_site * var_E
    chi = beta * N_site * var_M

    # Binder ç´¯ç§¯é‡ U
    m2 = np.mean(M ** 2)
    m4 = np.mean(M ** 4)
    if m2 <= 1e-15:
        U = 0.0  # éå¸¸æ¥è¿‘é«˜æ¸©æé™ / mâ‰ˆ0ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
    else:
        U = 1.0 - m4 / (3.0 * (m2 ** 2 + 1e-16))

    out = {
        "T": float(T),
        "h": float(h),
        "E_series": E,
        "M_series": M,
        "E_mean": E_mean,
        "M_mean": M_mean,
        "C": float(C),
        "chi": float(chi),
        "U": float(U),
        "n_samples": int(N_samples),
    }
    return out


def _load_thermo_from_metadata(meta_path: Path) -> Optional[Dict[str, Any]]:
    """
    ä» worker__metadata.json ä¸­è¯»å– thermo_stats / swap ä¿¡æ¯ã€‚

    æœŸæœ› JSON ä¸­åŒ…å«å­—æ®µï¼š
      - "thermo_stats": {
            "T_2.350000": {
                "T": 2.35,
                "C":...,"C_err":...,
                "chi":...,"chi_err":...,
                "U":...,
                "n_samples":...   # æˆ– "samples_per_temp"
            },
            ...
        }
      - "swap_summary" æˆ– "swap": {
            "rate": float,
            "attempts": [...],
            "accepts": [...],
            "pair_rates": [...]   # è‹¥å­˜åœ¨
        }

    è¿”å› dict æˆ– None:
        {
            "temps": np.array([...]),
            "C": np.array([...]),
            "C_err": np.array([...]),
            "chi": np.array([...]),
            "chi_err": np.array([...]),
            "U": np.array([...]),
            "n_samples": np.array([...], dtype=int),
            "swap": { ... }  # å¯èƒ½ä¸å­˜åœ¨
        }
    """
    if not meta_path.is_file():
        return None
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
    except Exception as exc:
        print(f"[warning] è¯»å– metadata {meta_path} å¤±è´¥: {exc}")
        return None

    thermo = meta.get("thermo_stats", None)
    if not isinstance(thermo, dict) or not thermo:
        return None

    temps: List[float] = []
    C_list: List[float] = []
    C_err_list: List[float] = []
    chi_list: List[float] = []
    chi_err_list: List[float] = []
    U_list: List[float] = []
    n_samples_list: List[int] = []

    for key, entry in thermo.items():
        if not isinstance(entry, dict):
            continue
        # T ä¼˜å…ˆç”¨ entry["T"]ï¼Œå¦åˆ™ä» key "T_2.350000" é‡Œè§£æ
        T_val = entry.get("T", None)
        if T_val is None:
            try:
                T_val = float(str(key).replace("T_", ""))
            except Exception:
                continue
        try:
            temps.append(float(T_val))
            C_list.append(float(entry.get("C", 0.0)))
            C_err_list.append(float(entry.get("C_err", 0.0)))
            chi_list.append(float(entry.get("chi", 0.0)))
            chi_err_list.append(float(entry.get("chi_err", 0.0)))
            U_list.append(float(entry.get("U", 0.0)))
            # å…¼å®¹ n_samples / samples_per_temp ä¸¤ç§å‘½å
            n_s = entry.get("n_samples", entry.get("samples_per_temp", 0))
            n_samples_list.append(int(n_s))
        except Exception:
            continue

    if not temps:
        return None

    # æŒ‰æ¸©åº¦æ’åº
    order = np.argsort(np.asarray(temps, dtype=float))
    temps_arr = np.asarray(temps, dtype=float)[order]
    C_arr = np.asarray(C_list, dtype=float)[order]
    C_err_arr = np.asarray(C_err_list, dtype=float)[order]
    chi_arr = np.asarray(chi_list, dtype=float)[order]
    chi_err_arr = np.asarray(chi_err_list, dtype=float)[order]
    U_arr = np.asarray(U_list, dtype=float)[order]
    n_samples_arr = np.asarray(n_samples_list, dtype=int)[order]

    swap_block = meta.get("swap_summary", None)
    if swap_block is None:
        swap_block = meta.get("swap", None)

    return {
        "temps": temps_arr,
        "C": C_arr,
        "C_err": C_err_arr,
        "chi": chi_arr,
        "chi_err": chi_err_arr,
        "U": U_arr,
        "n_samples": n_samples_arr,
        "swap": swap_block,
    }


# ------------------------------------------------------------------
# å¯¹å•ä¸ª worker çš„ HDF5 + JSON è¿›è¡Œåˆ†æå’Œä½œå›¾
# ------------------------------------------------------------------
def plot_worker_from_hdf5_group(worker_prefix: str,
                                files_to_process: List[Path],
                                meta_path: Optional[Path] = None,
                                out_prefix: Optional[Path] = None):
    """
    å¯¹æŸä¸ª worker çš„å…¨éƒ¨ HDF5 æ–‡ä»¶ï¼ˆä¸åŒ Tï¼‰è¿›è¡Œåˆ†æï¼š

      1. ä»æ‰€æœ‰ HDF5 é‡Œè¯»å‡º configsï¼Œè®¡ç®—ï¼š
         E_mean(T)ã€M_mean(T)ã€C(T)ã€chi(T)ã€U(T);
      2. è‹¥ä¼ å…¥ meta_path ä¸”å…¶ä¸­æœ‰ thermo_stats / swapï¼Œåˆ™å†ä» JSON ä¸­æå–ï¼š
         C/chi/U çš„ Bootstrap ä¼°è®¡åŠè¯¯å·®ï¼Œäº¤æ¢ç‡ç­‰ä¿¡æ¯ï¼›
      3. ç”Ÿæˆè‹¥å¹² png å›¾ï¼š
         - <out_prefix>_obs.png      : E/M/C/chi (raw, æ— è¯¯å·®)
         - <out_prefix>_binder.png   : Binder U(T) (raw)
         - <out_prefix>_thermo_meta.png : C/chi/U (æ¥è‡ª JSON, å¸¦è¯¯å·®æ¡, è‹¥æœ‰)
         - <out_prefix>_swap.png     : swap ç»Ÿè®¡ (è‹¥æœ‰)

    å‚æ•°ï¼š
      worker_prefix : worker åå­—ï¼ˆå‰ç¼€ï¼‰
      files_to_process : è¯¥ worker æ‰€æœ‰æ¸©åº¦çš„ HDF5 æ–‡ä»¶åˆ—è¡¨
      meta_path : å¯¹åº”çš„ JSON å…ƒæ•°æ®è·¯å¾„ï¼ˆå¯ä¸º Noneï¼‰
      out_prefix : è¾“å‡º png æ–‡ä»¶çš„å‰ç¼€ï¼ˆæ— æ‰©å±•åï¼‰ã€‚è‹¥ Noneï¼Œåˆ™ä½¿ç”¨
                   files_to_process[0].with_suffix("") ä½œä¸ºå‰ç¼€ã€‚
    """
    files_to_process = sorted(files_to_process,
                              key=lambda fp: _parse_worker_T_h_from_name(fp.name)[1]
                              if _parse_worker_T_h_from_name(fp.name) is not None
                              else 0.0)
    if not files_to_process:
        print(f"[warning] worker={worker_prefix} æ²¡æœ‰ä»»ä½• HDF5 æ–‡ä»¶å¯ç”¨ã€‚")
        return

    if out_prefix is None:
        out_prefix = files_to_process[0].with_suffix("")

    # å¯¹æ¯ä¸ª T æ–‡ä»¶è®¡ç®—ç»Ÿè®¡é‡
    results: List[dict] = []
    for fpath in files_to_process:
        ds = load_configs_hdf5(str(fpath), load_configs=True, load_obs=False)
        configs = np.asarray(ds["configs"])
        _, L, _ = configs.shape

        # ä¼˜å…ˆä» ds é‡Œæ‹¿ T / hï¼›æ²¡æœ‰åˆ™ä»æ–‡ä»¶åé‡Œè§£æ
        T_ds = ds.get("T", None)
        h_ds = ds.get("h", None)

        parsed = _parse_worker_T_h_from_name(fpath.name)
        if parsed is not None:
            _, T_from_name, h_from_name = parsed
        else:
            T_from_name, h_from_name = None, None

        T = T_ds if T_ds is not None else T_from_name
        h = h_ds if h_ds is not None else h_from_name
        if T is None:
            raise RuntimeError(f"æ— æ³•ä» {fpath.name} ä¸­è§£ææ¸©åº¦ T")
        if h is None:
            h = 0.0  # é»˜è®¤ h=0

        obs = _compute_observables_from_configs(configs, T=float(T), h=float(h))
        results.append(obs)

        print(
            f"[worker={worker_prefix}] {fpath.name}: "
            f"T={obs['T']:.6f}, h={obs['h']:.6f}, "
            f"n={obs['n_samples']}, "
            f"<E>={obs['E_mean']:.6f}, <m>={obs['M_mean']:.6f}, "
            f"C={obs['C']:.6f}, chi={obs['chi']:.6f}, U={obs['U']:.6f}"
        )

    # æŒ‰ T æ’åºå¹¶ç”» E(T)/M(T)/C(T)/chi(T)
    results_sorted = sorted(results, key=lambda d: d["T"])
    temps = np.array([r["T"] for r in results_sorted], dtype=float)
    E_mean = np.array([r["E_mean"] for r in results_sorted], dtype=float)
    M_mean = np.array([r["M_mean"] for r in results_sorted], dtype=float)
    C_vals = np.array([r["C"] for r in results_sorted], dtype=float)
    chi_vals = np.array([r["chi"] for r in results_sorted], dtype=float)
    U_vals = np.array([r["U"] for r in results_sorted], dtype=float)

    # ----------------- å›¾ 1ï¼šE, m, C, chi (raw) -----------------
    fig, ax = plt.subplots(2, 2, figsize=(10, 8))
    ax = ax.flatten()

    ax[0].plot(temps, E_mean, "o-", ms=3)
    ax[0].set_ylabel("E per spin")

    ax[1].plot(temps, M_mean, "o-", ms=3)
    ax[1].set_ylabel("m per spin")

    ax[2].plot(temps, C_vals, "o-", ms=3)
    ax[2].set_ylabel("C (raw)")

    ax[3].plot(temps, chi_vals, "o-", ms=3)
    ax[3].set_ylabel("chi (raw)")

    for a in ax:
        a.set_xlabel("T")
        a.axvline(2.269185, color="gray", ls="--", alpha=0.5)

    fig.suptitle(f"REMC observables (worker={worker_prefix})", fontsize=12)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    out_obs = out_prefix.with_name(out_prefix.name + "_obs.png")
    plt.savefig(out_obs, dpi=200)
    print("Saved plot:", out_obs)

    # ----------------- å›¾ 2ï¼šBinder U(T) (raw) -----------------
    if len(temps) > 0:
        fig2, ax2 = plt.subplots(figsize=(6, 4))
        ax2.plot(temps, U_vals, "o-", ms=3)
        ax2.set_xlabel("T")
        ax2.set_ylabel("Binder U (raw)")
        ax2.axvline(2.269185, color="gray", ls="--", alpha=0.5)
        ax2.set_title(f"Binder cumulant U(T) (worker={worker_prefix})")
        plt.tight_layout()
        out_binder = out_prefix.with_name(out_prefix.name + "_binder.png")
        plt.savefig(out_binder, dpi=200)
        print("Saved plot:", out_binder)

    # ----------------- å›¾ 3/4ï¼šä» metadata.json è¯»å– thermo_stats + swap -----------------
    meta_info = None
    if meta_path is not None:
        meta_info = _load_thermo_from_metadata(meta_path)

    # 3.1 thermo_stats: C/Ï‡/U å¸¦è¯¯å·®
    if meta_info is not None:
        temps_m = meta_info["temps"]
        C_m = meta_info["C"]
        C_err_m = meta_info["C_err"]
        chi_m = meta_info["chi"]
        chi_err_m = meta_info["chi_err"]
        U_m = meta_info["U"]

        fig3, ax3 = plt.subplots(3, 1, figsize=(6, 9), sharex=True)
        ax3[0].errorbar(temps_m, C_m, yerr=C_err_m, fmt="o-", ms=3)
        ax3[0].set_ylabel("C")
        ax3[0].axvline(2.269185, color="gray", ls="--", alpha=0.5)

        ax3[1].errorbar(temps_m, chi_m, yerr=chi_err_m, fmt="o-", ms=3)
        ax3[1].set_ylabel("chi")
        ax3[1].axvline(2.269185, color="gray", ls="--", alpha=0.5)

        ax3[2].plot(temps_m, U_m, "o-", ms=3)
        ax3[2].set_ylabel("Binder U")
        ax3[2].set_xlabel("T")
        ax3[2].axvline(2.269185, color="gray", ls="--", alpha=0.5)

        fig3.suptitle(f"Thermo observables from metadata (worker={worker_prefix})", fontsize=12)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        out_thermo = out_prefix.with_name(out_prefix.name + "_thermo_meta.png")
        plt.savefig(out_thermo, dpi=200)
        print("Saved plot:", out_thermo)

        # 3.2 swap ç»Ÿè®¡
        swap_block = meta_info.get("swap", None)
        if isinstance(swap_block, dict):
            rate_global = float(swap_block.get("rate", 0.0))
            pair_rates = swap_block.get("pair_rates", None)
            if pair_rates is not None:
                pair_rates = np.asarray(pair_rates, dtype=float)
            else:
                # è‹¥æ²¡æœ‰ pair_ratesï¼Œä½†æœ‰ attempts/acceptsï¼Œä¹Ÿå¯ä»¥è®¡ç®—ä¸€ä¸‹
                attempts = np.asarray(swap_block.get("attempts", []), dtype=float)
                accepts = np.asarray(swap_block.get("accepts", []), dtype=float)
                if attempts.size and accepts.size and attempts.size == accepts.size:
                    with np.errstate(divide="ignore", invalid="ignore"):
                        pr = np.where(attempts > 0, accepts / attempts, 0.0)
                    pair_rates = pr
                else:
                    pair_rates = np.array([])

            fig4, ax4 = plt.subplots(figsize=(6, 4))
            if pair_rates.size > 0:
                # å– pair ä¸­å¿ƒæ¸©åº¦ä½œä¸ºæ¨ªåæ ‡ï¼Œè‹¥é•¿åº¦åŒ¹é…çš„è¯
                temps_mid = (temps_m[:-1] + temps_m[1:]) / 2.0
                if temps_mid.size == pair_rates.size:
                    ax4.plot(temps_mid, pair_rates, "o-", ms=3, label="pair swap rate")
                    ax4.set_xlabel("mid T of pair")
                else:
                    ax4.plot(np.arange(pair_rates.size), pair_rates, "o-", ms=3, label="pair swap rate")
                    ax4.set_xlabel("pair index")
            else:
                ax4.set_xlabel("pair index")

            ax4.axhline(rate_global, color="red", ls="--",
                        label=f"global rate={rate_global:.3f}")
            ax4.set_ylabel("swap rate")
            ax4.set_title(f"Swap statistics (worker={worker_prefix})")
            ax4.legend()
            plt.tight_layout()

            out_swap = out_prefix.with_name(out_prefix.name + "_swap.png")
            plt.savefig(out_swap, dpi=200)
            print("Saved plot:", out_swap)


# ------------------------------------------------------------------
# tnn_L*.npz çš„æ—§è·¯å¾„ä¿ç•™
# ------------------------------------------------------------------
def plot_from_tnn_npz(npz_path: Path, out_png: Optional[Path] = None):
    data = np.load(npz_path)
    T = data["temperatures"]
    E = data["E"]
    M = data["M"]
    C = data["C"]
    chi = data["chi"]

    fig, ax = plt.subplots(2, 2, figsize=(10, 8))
    ax = ax.flatten()
    ax[0].plot(T, E, "o-")
    ax[0].set_ylabel("E")
    ax[1].plot(T, M, "o-")
    ax[1].set_ylabel("M")
    ax[2].plot(T, C, "o-")
    ax[2].set_ylabel("C")
    ax[3].plot(T, chi, "o-")
    ax[3].set_ylabel("chi")

    for a in ax:
        a.set_xlabel("T")
        a.axvline(2.269185, color="gray", ls="--", alpha=0.5)

    plt.tight_layout()
    if out_png is not None:
        plt.savefig(out_png, dpi=200)
        print("Saved plot to", out_png)
    else:
        plt.show()


# ------------------------------------------------------------------
# ä¸»å…¥å£ï¼šç»™ä¸€ä¸ª remc_simulator è¾“å‡ºç›®å½•ï¼Œè‡ªåŠ¨è¯†åˆ«å¹¶ä½œå›¾
# ------------------------------------------------------------------
def analyze_remc_output_dir(dir_path: Path):
    """
    ç»™ remc_simulator / GPU_REMC_Simulator çš„è¾“å‡ºç›®å½•ï¼Œæ¯”å¦‚ï¼š

        examples/runs/L64_from_yaml/

    ç›®å½•ä¸­åŒ…å«ï¼š
      - <worker>__latt_T_..._h....h5
      - <worker>__metadata.json

    æœ¬å‡½æ•°ä¼šï¼š
      1. æ‰¾åˆ°æ‰€æœ‰åŒ¹é… HDF5ï¼ŒæŒ‰ worker åˆ†ç»„ï¼›
      2. å¯¹æ¯ä¸ª workerï¼Œè°ƒç”¨ plot_worker_from_hdf5_group(...) è¿›è¡Œä½œå›¾ã€‚
    """
    dir_path = dir_path.resolve()
    if not dir_path.is_dir():
        raise NotADirectoryError(dir_path)

    # æ”¶é›†è¯¥ç›®å½•ä¸‹æ‰€æœ‰ç¬¦åˆå‘½åçº¦å®šçš„ HDF5
    groups: Dict[str, List[Path]] = defaultdict(list)
    for f in dir_path.iterdir():
        if not f.is_file():
            continue
        if not f.name.endswith(".h5"):
            continue
        parsed = _parse_worker_T_h_from_name(f.name)
        if parsed is None:
            continue
        worker, T, h = parsed
        groups[worker].append(f)

    if not groups:
        print(f"[warning] ç›®å½• {dir_path} ä¸‹æ²¡æœ‰åŒ¹é…æ¨¡å¼ 'xxx__latt_T_..._h....h5' çš„ HDF5 æ–‡ä»¶ã€‚")
        return

    # å¯¹æ¯ä¸ª worker åˆ†åˆ«ä½œå›¾
    for worker, files in groups.items():
        meta_path = dir_path / f"{worker}__metadata.json"
        print(f"[dir] worker='{worker}' å‘ç° {len(files)} ä¸ªæ¸©åº¦æ–‡ä»¶ï¼Œ"
              f"metadata={'å­˜åœ¨' if meta_path.is_file() else 'ä¸å­˜åœ¨'}")

        # è¾“å‡ºå‰ç¼€ï¼šåœ¨ç›®å½•ä¸‹ç”Ÿæˆ <worker>_remc_summary_*.png
        # ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ç¬¬ä¸€ä¸ª HDF5 çš„åå­—åšå‰ç¼€
        out_prefix = (dir_path / f"{worker}__remc_summary").with_suffix("")

        plot_worker_from_hdf5_group(
            worker_prefix=worker,
            files_to_process=files,
            meta_path=meta_path if meta_path.is_file() else None,
            out_prefix=out_prefix,
        )


# ------------------------------------------------------------------
# main
# ------------------------------------------------------------------
def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "path",
        help="remc è¾“å‡ºç›®å½• / å•ä¸ª HDF5 æ–‡ä»¶ / tnn_L*.npz",
    )
    args = parser.parse_args()

    path = Path(args.path)

    if path.is_dir():
        # ç›®å½•æ¨¡å¼ï¼šå¯¹ç›®å½•ä¸‹æ‰€æœ‰ worker è‡ªåŠ¨ä½œå›¾
        analyze_remc_output_dir(path)
        return

    # å•æ–‡ä»¶æ¨¡å¼ï¼šä¿æŒåŸæ¥çš„é€»è¾‘
    if path.suffix.lower() == ".npz":
        plot_from_tnn_npz(path, out_png=path.with_suffix(".png"))
    else:
        # å•ä¸ª HDF5ï¼šå½“æˆåªæœ‰ä¸€ä¸ª worker çš„ç›®å½•æ¥ç”¨
        parsed = _parse_worker_T_h_from_name(path.name)
        if parsed is None:
            raise RuntimeError(
                f"å•æ–‡ä»¶æ¨¡å¼ä¸‹ï¼ŒHDF5 æ–‡ä»¶åéœ€ç¬¦åˆ 'xxx__latt_T_..._h....h5'ï¼Œå½“å‰ä¸º {path.name}"
            )
        worker, T0, h0 = parsed
        meta_path = path.parent / f"{worker}__metadata.json"
        plot_worker_from_hdf5_group(
            worker_prefix=worker,
            files_to_process=[path],
            meta_path=meta_path if meta_path.is_file() else None,
            out_prefix=path.with_suffix(""),
        )


if __name__ == "__main__":
    main()

```
ä½¿ç”¨æ–¹æ³•ï¼š
a.é’ˆå¯¹æ•´ä¸ª remc è¾“å‡ºç›®å½•
````
python examples/load_and_analyze.py /path/to/remc_output_dir
````
è¾“å‡ºæ¯ä¸ª worker ç”Ÿæˆä¸€ç»„ï¼š
````

worker__remc_summary_obs.png

worker__remc_summary_binder.png

worker__remc_summary_thermo_meta.pngï¼ˆè‹¥ JSON é‡Œæœ‰ thermo_statsï¼‰

worker__remc_summary_swap.pngï¼ˆè‹¥ JSON é‡Œæœ‰ swap/swap_summaryï¼‰
````
b.å¦‚æœä½ åªæƒ³åˆ†ææŸä¸€ä¸ª HDF5ï¼š
````
python examples/load_and_analyze.py /path/to/remc_output_dir/worker__latt_T_2.350000_h0.000000.h5
````

c.tnn_L.npzï¼šï¼ˆé€‚åº”gpu çš„lattice_saved = False æƒ…å†µä¸‹çš„å…ƒæ•°æ®è¾“å‡ºä¸è®°å½•ï¼‰
````
python examples/load_and_analyze.py /path/to/tnn_L64.npz
````

### 3.2 å°å‹æ¼”ç¤ºï¼šREMC â†’ FSSAnalyzer â†’ Tc / ä¸´ç•ŒæŒ‡æ•° / æ•°æ®å¡Œç¼©
```python
# examples/analysis/remc_fss_demo.py
"""
ç‰©ç†ç‰ˆç¤ºä¾‹ï¼šREMC â†’ FSSAnalyzer â†’ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©

æ³¨æ„ï¼š
- è¿™æ˜¯â€œç›¸å¯¹ç‰©ç†é è°±â€çš„ demoï¼Œè€Œä¸æ˜¯å¿«é€Ÿå•å…ƒæµ‹è¯•ã€‚
- é»˜è®¤å‚æ•°ä¼šæ¯” demo_remc_fss_pipeline.py è·‘å¾—ä¹…å¾ˆå¤šï¼ˆè§†æœºå™¨æ€§èƒ½ï¼Œå¯èƒ½æ˜¯åˆ†é’Ÿçº§ç”šè‡³æ›´é•¿ï¼‰ã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Dict, Any

import numpy as np

# ---- ä¿è¯å¯ä»¥ç›´æ¥ä»æºç å¯¼å…¥ ising_fss ----
ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds
from ising_fss.analysis.fss_analyzer import FSSAnalyzer


# -----------------------------
# 1. å•ä¸ª L çš„ REMC + åˆ†æ
# -----------------------------

def run_single_L(
    L: int,
    T_min: float,
    T_max: float,
    num_replicas: int = 16,
    equil_steps: int = 20_000,
    prod_steps: int = 80_000,
    thin: int = 20,
    exchange_interval: int = 5,
    algorithm: str = "metropolis_sweep",
) -> Dict[float, Dict[str, Any]]:
    """
    è·‘å•ä¸€æ™¶æ ¼å°ºå¯¸ L çš„ REMCï¼Œå¹¶è¿”å›ï¼š
        { T: {obs_dict}, ... }

    å…¶ä¸­ obs_dict ä¸­ä¼šå°½å¯èƒ½åŒ…å«ï¼š
        - E, M, C, chi, U åŠå…¶è¯¯å·®ï¼š
            E_err, M_err, C_err, chi_err, U_err
        - ä»¥åŠå¯é€‰çš„æ ·æœ¬æ•°ç»„ï¼š
            E_samples, M_samples, C_samples, chi_samples, ...
    """
    print(
        f"\n=== è¿è¡Œ REMC (ç‰©ç†ç‰ˆ): L={L}, "
        f"Tâˆˆ[{T_min}, {T_max}], replicas={num_replicas}, algo={algorithm} ==="
    )

    replica_seeds = make_replica_seeds(master_seed=10_000 + L, n_replicas=num_replicas)

    sim = HybridREMCSimulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        replica_seeds=replica_seeds,
        algorithm=algorithm,
        h=0.0,
    )

    sim.run(
        equilibration_steps=equil_steps,
        production_steps=prod_steps,
        exchange_interval=exchange_interval,
        thin=thin,
        save_lattices=False,  # è¿™é‡Œåªå…³å¿ƒç»Ÿè®¡é‡ï¼Œä¸è½ç›˜æ™¶æ ¼
        verbose=False,
    )

    res = sim.analyze(verbose=False)

    temp_map: Dict[float, Dict[str, Any]] = {}

    # æ ‡é‡å‡å€¼
    mean_keys = ["E", "M", "C", "chi", "U"]
    # æ ‡å‡†è¯¯å·®
    err_keys = ["E_err", "M_err", "C_err", "chi_err", "U_err"]
    # æ ·æœ¬æ•°ç»„
    sample_keys = [
        "E_samples",
        "M_samples",
        "C_samples",
        "chi_samples",
    ]

    for key, val in res.items():
        if not isinstance(key, str) or not key.startswith("T_"):
            continue
        try:
            T = float(key.split("_", 1)[1])
        except Exception:
            continue

        obs: Dict[str, Any] = {}

        # ---- 1) å‡å€¼ ----
        for name in mean_keys:
            if name in val:
                try:
                    v = float(val[name])
                    if np.isfinite(v):
                        obs[name] = v
                except Exception:
                    continue

        # ---- 2) è¯¯å·®æ¡ ----
        for name in err_keys:
            if name in val:
                try:
                    v = float(val[name])
                    if not np.isfinite(v):
                        continue
                    # åŸå§‹ *_err ä¿ç•™
                    obs[name] = v

                    # å…³é”®ä¸€æ­¥ï¼šå†å¤åˆ¶ä¸€ä»½æˆ *_stderrï¼Œç»™ FSSAnalyzer ç”¨
                    # ä¾‹å¦‚ chi_err -> chi_stderr, C_err -> C_stderr
                    if name.endswith("_err"):
                        base = name[:-4]  # å»æ‰ "_err"
                        stderr_key = f"{base}_stderr"
                        obs[stderr_key] = v
                except Exception:
                    continue

        # ---- 3) æ ·æœ¬æ•°ç»„ ----
        for name in sample_keys:
            if name in val:
                try:
                    arr = np.asarray(val[name], dtype=float)
                    if arr.size > 0:
                        obs[name] = arr
                except Exception:
                    continue

        # ---- 4) è¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚ n_samplesï¼‰----
        for aux_key in ["n_samples", "samples"]:
            if aux_key in val:
                try:
                    obs[aux_key] = int(val[aux_key])
                except Exception:
                    pass

        temp_map[T] = obs

    print("  æ”¶åˆ°æ¸©åº¦ç‚¹æ•°é‡:", len(temp_map))
    return temp_map


# -----------------------------
# 2. å¤šä¸ª L çš„ç»“æœæ‹¼æˆ FSS è¾“å…¥
# -----------------------------
def build_fss_results_for_sizes(
    L_list,
    T_min: float,
    T_max: float,
    num_replicas: int = 16,
    equil_steps: int = 20_000,
    prod_steps: int = 80_000,
    thin: int = 20,
    exchange_interval: int = 5,
    algorithm: str = "metropolis_sweep",
):
    """
    è¿”å›ç»“æ„ï¼š
        results[L][T] = {obs_dict}

    obs_dict é‡ŒåŒ…å«ï¼š
        - E, M, C, chi, U
        - åŠå…¶è¯¯å·®ï¼šE_err, M_err, C_err, chi_err, U_err
        - ä»¥åŠå…¼å®¹ FSSAnalyzer çš„ï¼šE_stderr, M_stderr, C_stderr, chi_stderr, U_stderr
        - ä»¥åŠå¯é€‰çš„ *_samples æ•°ç»„ï¼ˆè‹¥ analyze() æä¾›ï¼‰ã€‚
    """
    all_results: Dict[int, Dict[float, Dict[str, Any]]] = {}
    for L in L_list:
        all_results[int(L)] = run_single_L(
            L=L,
            T_min=T_min,
            T_max=T_max,
            num_replicas=num_replicas,
            equil_steps=equil_steps,
            prod_steps=prod_steps,
            thin=thin,
            exchange_interval=exchange_interval,
            algorithm=algorithm,
        )
    return all_results


# -----------------------------
# å·¥å…·å‡½æ•°ï¼šæŒ‰æ¡ç›®æ¢è¡Œæ‰“å° Tc_est ç»“æœ
# -----------------------------
def _pretty_print_Tc_est(label: str, est: Dict[str, Any]) -> None:
    """
    æŒ‰æ¡ç›®ï¼ˆkeyï¼‰é€è¡Œæ‰“å° estimate_Tc è¿”å›çš„å­—å…¸ï¼Œ
    å¯¹ crossings / weights / pairs åšç®€å•å±•å¼€ï¼Œä¾¿äºé˜…è¯»ã€‚
    """
    print(f"[INFO] {label} ç»“æœ:")

    if not isinstance(est, dict):
        print(f"  {est}")
        return

    # å…ˆæ‰“å‡ ä¸ªå¸¸ç”¨æ ‡é‡
    for key in ("Tc", "var", "std"):
        if key in est:
            print(f"  {key}: {est[key]}")

    # æ‰“å°æƒé‡
    if "weights" in est:
        print("  weights:")
        try:
            for w in est["weights"]:
                print(f"    - {w}")
        except TypeError:
            print(f"    {est['weights']}")

    # æ‰“å° (L1, L2) é…å¯¹
    if "pairs" in est:
        print("  pairs:")
        try:
            for pair in est["pairs"]:
                try:
                    L1, L2 = pair
                    print(f"    - ({L1}, {L2})")
                except Exception:
                    print(f"    - {pair}")
        except TypeError:
            print(f"    {est['pairs']}")

    # æ‰“å° crossings è¯¦æƒ…
    if "crossings" in est:
        print("  crossings:")
        try:
            for c in est["crossings"]:
                # å°è¯•æŒ‰ PairCrossing çš„å±æ€§æ¥æ‰“å°
                try:
                    L1 = getattr(c, "L1", None)
                    L2 = getattr(c, "L2", None)
                    Tc_c = getattr(c, "Tc", None)
                    slope_diff = getattr(c, "slope_diff", None)
                    bracket = getattr(c, "bracket", None)
                    method = getattr(c, "method", "")
                    note = getattr(c, "note", "")

                    line = "    - "
                    if L1 is not None and L2 is not None:
                        line += f"L1={L1}, L2={L2}, "
                    if Tc_c is not None:
                        try:
                            line += f"Tc={Tc_c:.6f}, "
                        except Exception:
                            line += f"Tc={Tc_c}, "
                    if slope_diff is not None:
                        try:
                            line += f"slope_diff={slope_diff:.3f}, "
                        except Exception:
                            line += f"slope_diff={slope_diff}, "
                    if bracket is not None:
                        line += f"bracket={bracket}, "
                    if method:
                        line += f"method={method}"
                    if note:
                        line += f", note={note}"
                    print(line)
                except Exception:
                    # æ‰“å°å¤±è´¥å°±ç›´æ¥ print å¯¹è±¡
                    print(f"    - {c}")
        except TypeError:
            print(f"    {est['crossings']}")

    # å…¶ä½™é”®ï¼ˆå¦‚æœæœ‰ï¼‰ä¹Ÿé€è¡Œæ‰“å°ï¼Œé¿å…é—æ¼
    for key, value in est.items():
        if key in ("Tc", "var", "std", "weights", "pairs", "crossings"):
            continue
        print(f"  {key}: {value}")


# -----------------------------
# 3. FSS åˆ†æï¼ˆæ›´åâ€œç‰©ç†ç‰ˆâ€ï¼‰
# -----------------------------
def run_fss_analysis(results: Dict[int, Dict[float, Dict[str, Any]]]):
    print("\n=== æ„å»º FSSAnalyzer (ç‰©ç†ç‰ˆ) ===")

    analyzer = FSSAnalyzer(results, Tc_theory=2.269185)

    # -------- 1) Binder U çš„äº¤å‰ç‚¹ â†’ Tc ä¼°è®¡ --------
    Tc_val = None
    try:
        Tc_est = analyzer.estimate_Tc("U")
        if isinstance(Tc_est, dict):
            Tc_val = float(Tc_est.get("Tc", None))
            # è¿™é‡Œæ”¹æˆæŒ‰æ¡ç›®æ¢è¡Œæ‰“å°
            _pretty_print_Tc_est("estimate_Tc('U')", Tc_est)
        else:
            Tc_val = float(Tc_est)
            print(f"[INFO] estimate_Tc('U') å¾—åˆ° Tc â‰ˆ {Tc_val:.6f}")
    except Exception as e:
        print("[WARN] estimate_Tc('U') å¤±è´¥:", e)

    if Tc_val is None:
        Tc_val = 2.269185
        print(f"[INFO] ä½¿ç”¨ç†è®º Tc = {Tc_val:.6f} ä½œä¸ºåç»­æ‹ŸåˆåŸºå‡†")
    else:
        print(f"[INFO] ä¼°è®¡ Tc â‰ˆ {Tc_val:.6f} (ç†è®ºå€¼ Tcâ‰ˆ2.269185)")

    # -------- 2) æå– Î³/Î½ ï¼ˆç”¨ Ï‡ çš„ FSS æ ‡åº¦ï¼‰ --------
    gamma_over_nu = None
    try:
        expo = analyzer.extract_critical_exponents(
            observable="chi",
            Tc_hint=Tc_val,
            fit_nu=False,  # Î½ å·²çŸ¥ä¸º 1 çš„æƒ…å½¢ä¸‹ï¼Œåªæ‹Ÿåˆ Î³/Î½ æ›´ç¨³ä¸€äº›
        )
        print("exponents (from chi):", expo)

        for k in ["gamma_over_nu", "exponent_ratio", "exponent"]:
            if k in expo:
                gamma_over_nu = float(expo[k])
                print(f"[INFO] è¯†åˆ«åˆ° {k} â‰ˆ {gamma_over_nu:.4f}")
                break
    except TypeError:
        expo = analyzer.extract_critical_exponents("chi")
        print("exponents (from chi):", expo)
        for k in ["gamma_over_nu", "exponent_ratio", "exponent"]:
            if k in expo:
                gamma_over_nu = float(expo[k])
                print(f"[INFO] è¯†åˆ«åˆ° {k} â‰ˆ {gamma_over_nu:.4f}")
                break
    except Exception as e:
        print("[WARN] æå–ä¸´ç•ŒæŒ‡æ•°å¤±è´¥:", e)

    if gamma_over_nu is not None:
        print(
            "[INFO] ç†è®ºå€¼ Î³/Î½ â‰ˆ 1.75; "
            f"å½“å‰æ‹Ÿåˆå¾—åˆ° Î³/Î½ â‰ˆ {gamma_over_nu:.4f}"
        )
        if gamma_over_nu < 0:
            print("[WARN] Î³/Î½ < 0 æ˜æ˜¾è¿èƒŒç‰©ç†å¸¸è¯†ï¼Œè¯´æ˜é‡‡æ ·æˆ–æ‹Ÿåˆè¿˜æœ‰é—®é¢˜ã€‚")
    else:
        print("[WARN] æœªèƒ½ä» expo ä¸­è¯†åˆ«å‡º Î³/Î½ï¼Œåç»­ data collapse å°†ä½¿ç”¨ç†è®ºå€¼ã€‚")
        gamma_over_nu = 1.75

    # -------- 3) æ•°æ®å¡Œç¼©ï¼ˆchiï¼‰ --------
    print("\n=== chi æ•°æ®å¡Œç¼© (ç‰©ç†ç‰ˆ) ===")
    if not hasattr(analyzer, "data_collapse"):
        print("[INFO] å½“å‰ FSSAnalyzer æœªå®ç° data_collapseï¼Œè·³è¿‡è¯¥æ­¥éª¤ã€‚")
        return

    try:
        collapse = analyzer.data_collapse(
            observable="chi",
            Tc=Tc_val,
            nu=1.0,                # 2D Ising çš„ç†è®º Î½ = 1
            exponent_ratio=gamma_over_nu,
        )
        print("data_collapse keys:", list(collapse.keys()))
        if "score" in collapse:
            print(f"collapse score â‰ˆ {collapse['score']:.6g}")
            print("ï¼ˆscore è¶Šå°é€šå¸¸ä»£è¡¨å¡Œç¼©è´¨é‡è¶Šå¥½ï¼Œä»…ä¾›ç›¸å¯¹æ¯”è¾ƒï¼‰")
    except Exception as e:
        print("[WARN] data_collapse è°ƒç”¨å¤±è´¥:", e)


# -----------------------------
# 4. mainï¼šä¸€é”®è·‘â€œç‰©ç†ç‰ˆâ€ç®¡çº¿
# -----------------------------
def main():
    # ---- è¿™é‡Œæ˜¯å¯ä»¥æŒ‰éœ€è¦è°ƒèŠ‚çš„â€œç‰©ç†å‚æ•°â€ ----
    L_list = [16, 32, 64]    # å¦‚æœæœºå™¨ç»™åŠ›å¯ä»¥åŠ åˆ° 128
    T_min, T_max = 2.1, 2.5  # æŠŠæ¸©åº¦åŒºé—´æ”¶çª„åˆ°ä¸´ç•Œé™„è¿‘
    num_replicas = 16        # æ¸©åº¦ç‚¹æ•°é‡ï¼ˆæ¯ä¸ª L ä¸Šçš„ T æ•°ç›®ï¼‰

    equil_steps = 20_000     # å¹³è¡¡ steps
    prod_steps = 80_000      # é‡‡æ · steps
    thin = 20                # æ¯éš” thin sweeps å–ä¸€ä¸ªæ ·æœ¬
    exchange_interval = 5    # æ¯ 5 sweeps å°è¯•ä¸€æ¬¡äº¤æ¢

    print("=" * 70)
    print("ç‰©ç†ç‰ˆç¤ºä¾‹ï¼šREMC â†’ FSSAnalyzer â†’ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©")
    print("=" * 70)
    print(
        f"å‚æ•°æ¦‚è§ˆï¼šL_list={L_list}, Tâˆˆ[{T_min},{T_max}], "
        f"replicas={num_replicas}, equil={equil_steps}, prod={prod_steps}, thin={thin}"
    )

    results = build_fss_results_for_sizes(
        L_list=L_list,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        equil_steps=equil_steps,
        prod_steps=prod_steps,
        thin=thin,
        exchange_interval=exchange_interval,
        algorithm="metropolis_sweep",
    )

    print("\n=== results é¢„è§ˆ ===")
    for L, Tmap in results.items():
        print("L=", L, "| #T =", len(Tmap))

    run_fss_analysis(results)


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š

````
(base)  ğŸ”¥ $ python remc_fss_pipeline_demo0.py 
======================================================================
ç‰©ç†ç‰ˆç¤ºä¾‹ï¼šREMC â†’ FSSAnalyzer â†’ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©
======================================================================
å‚æ•°æ¦‚è§ˆï¼šL_list=[16, 32, 64], Tâˆˆ[2.1,2.5], replicas=16, equil=20000, prod=80000, thin=20

=== è¿è¡Œ REMC (ç‰©ç†ç‰ˆ): L=16, Tâˆˆ[2.1, 2.5], replicas=16, algo=metropolis_sweep ===
  æ”¶åˆ°æ¸©åº¦ç‚¹æ•°é‡: 16

=== è¿è¡Œ REMC (ç‰©ç†ç‰ˆ): L=32, Tâˆˆ[2.1, 2.5], replicas=16, algo=metropolis_sweep ===
  æ”¶åˆ°æ¸©åº¦ç‚¹æ•°é‡: 16

=== è¿è¡Œ REMC (ç‰©ç†ç‰ˆ): L=64, Tâˆˆ[2.1, 2.5], replicas=16, algo=metropolis_sweep ===
  æ”¶åˆ°æ¸©åº¦ç‚¹æ•°é‡: 16

=== results é¢„è§ˆ ===
L= 16 | #T = 16
L= 32 | #T = 16
L= 64 | #T = 16

=== æ„å»º FSSAnalyzer (ç‰©ç†ç‰ˆ) ===
[INFO] estimate_Tc('U') ç»“æœ:
  Tc: 2.2616322349964766
  var: 1.1264680241881608e-05
  std: 0.0033562896540497824
  weights:
    - 0.5015754252200023
    - 1.3973103117372652
    - 0.8769157952135199
  pairs:
    - (16, 32)
    - (16, 64)
    - (32, 64)
  crossings:
    - L1=16, L2=32, Tc=2.268248, slope_diff=0.502, bracket=(2.2675146771037182, 2.268297455968689), method=bisection
    - L1=16, L2=64, Tc=2.261284, slope_diff=1.397, bracket=(2.261252446183953, 2.2620352250489235), method=bisection
    - L1=32, L2=64, Tc=2.258403, slope_diff=0.877, bracket=(2.2581213307240704, 2.258904109589041), method=bisection
[INFO] ä¼°è®¡ Tc â‰ˆ 2.261632 (ç†è®ºå€¼ Tcâ‰ˆ2.269185)
exponents (from chi): {'Tc_used': 2.2616322349964766, 'gamma_over_nu': 1.8324530469170346, 'nu': 1.0, 'intercept': -0.9024085248605331, 'sizes_used': [16, 32, 64]}
[INFO] è¯†åˆ«åˆ° gamma_over_nu â‰ˆ 1.8325
[INFO] ç†è®ºå€¼ Î³/Î½ â‰ˆ 1.75; å½“å‰æ‹Ÿåˆå¾—åˆ° Î³/Î½ â‰ˆ 1.8325

=== chi æ•°æ®å¡Œç¼© (ç‰©ç†ç‰ˆ) ===
data_collapse keys: ['observable', 'Tc', 'nu', 'exponent_ratio', 'curves', 'score', 'success', 16, 32, 64]
collapse score â‰ˆ 0.00018497
ï¼ˆscore è¶Šå°é€šå¸¸ä»£è¡¨å¡Œç¼©è´¨é‡è¶Šå¥½ï¼Œä»…ä¾›ç›¸å¯¹æ¯”è¾ƒï¼‰
````

### 3.3 ä¸ºå¼ é‡ç½‘ç»œ (TNN) / TNR ç”Ÿæˆå¤š L çš„çƒ­åŠ›å­¦ç»Ÿè®¡æ•°æ® (NPZ)ã€‚
```python
# examples/analysis/tnn_data_generation.py
"""
ä¸ºå¼ é‡ç½‘ç»œ (TNN) / TNR ç”Ÿæˆå¤š L çš„çƒ­åŠ›å­¦ç»Ÿè®¡æ•°æ® (NPZ)ã€‚
"""

from __future__ import annotations

import sys
import time
from pathlib import Path
from typing import Dict, Any, Mapping, List, Tuple

import numpy as np
import matplotlib.pyplot as plt

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.parallel import across_L
from ising_fss.utils.logger import ExperimentLogger
from ising_fss.visualization.styles import publication_style


def _iter_temp_items(
    data_L: Mapping[Any, Mapping[str, Any]]
) -> List[Tuple[float, Any]]:
    items: List[Tuple[float, Any]] = []
    for k in data_L.keys():
        if isinstance(k, (int, float)):
            T_val = float(k)
        elif isinstance(k, str):
            if k.startswith("T_"):
                try:
                    T_val = float(k[2:])
                except ValueError:
                    continue
            else:
                try:
                    T_val = float(k)
                except ValueError:
                    continue
        else:
            continue
        items.append((T_val, k))
    items.sort(key=lambda x: x[0])
    return items


def export_tnn_data(results: Dict[int, Dict[float, Dict]], out_dir: Path):
    out_dir.mkdir(parents=True, exist_ok=True)
    for L, data_L in results.items():
        if not data_L:
            continue
        if isinstance(data_L, dict) and "error" in data_L:
            print(f"âš ï¸ è·³è¿‡ L={L} (æ¨¡æ‹Ÿå¤±è´¥: {data_L['error']})")
            continue
        temp_items = _iter_temp_items(data_L)
        if not temp_items:
            print(f"âš ï¸ L={L} æœªæ‰¾åˆ°æ¸©åº¦é”®ï¼Œè·³è¿‡")
            continue

        T_vals = [T for T, _ in temp_items]
        n_T = len(T_vals)
        arrays: Dict[str, np.ndarray] = {
            "temperatures": np.asarray(T_vals, dtype=np.float64),
            "L": np.int64(L),
        }
        keys = ["E", "M", "C", "chi", "U", "E_err", "M_err", "C_err", "chi_err"]
        for name in keys:
            arr = np.full(n_T, np.nan, dtype=np.float64)
            for i, (_T, orig) in enumerate(temp_items):
                try:
                    val = data_L[orig].get(name, np.nan)
                except Exception:
                    val = np.nan
                arr[i] = float(val) if val is not None else np.nan
            arrays[name] = arr

        fname = out_dir / f"tnn_L{L}.npz"
        np.savez_compressed(fname, **arrays)
        print(f"âœ“ å¯¼å‡º L={L}: {fname}")


def plot_overview(results: Dict[int, Dict], out_path: str):
    with publication_style():
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        L_list = sorted(results.keys())
        colors = plt.cm.viridis(np.linspace(0, 1, len(L_list)))
        for idx, L in enumerate(L_list):
            data_L = results[L]
            if isinstance(data_L, dict) and "error" in data_L:
                continue
            temp_items = _iter_temp_items(data_L)
            if not temp_items:
                continue
            Ts = [T for T, _orig in temp_items]
            Es = [data_L[orig].get("E", np.nan) for _, orig in temp_items]
            Ms = [data_L[orig].get("M", np.nan) for _, orig in temp_items]
            Cs = [data_L[orig].get("C", np.nan) for _, orig in temp_items]
            Xs = [data_L[orig].get("chi", np.nan) for _, orig in temp_items]
            kw = dict(marker=".", ls="-", color=colors[idx], label=f"L={L}", alpha=0.8)
            axes[0].plot(Ts, Es, **kw)
            axes[1].plot(Ts, Ms, **kw)
            axes[2].plot(Ts, Cs, **kw)
            axes[3].plot(Ts, Xs, **kw)

        axes[0].set_ylabel("E")
        axes[1].set_ylabel("M")
        axes[2].set_ylabel("C")
        axes[3].set_ylabel("chi")
        for ax in axes:
            ax.set_xlabel("T")
            ax.legend(fontsize="small")
            ax.axvline(2.269185, color="gray", ls="--", alpha=0.5)
        plt.tight_layout()
        plt.savefig(out_path, dpi=300)
        print("ğŸ“Š æ¦‚è§ˆå›¾å·²ä¿å­˜:", out_path)


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--L_list", type=int, nargs="+", default=[16, 32, 64])
    parser.add_argument("--T_min", type=float, default=2.0)
    parser.add_argument("--T_max", type=float, default=2.6)
    parser.add_argument("--n_T", type=int, default=32)
    parser.add_argument("--outdir", default="data_tnn")
    parser.add_argument("--algo", default="wolff")
    parser.add_argument("--quick", action="store_true")
    parser.add_argument("--high_precision", action="store_true")
    args = parser.parse_args()

    equil, prod, thin = 5000, 20000, 10
    if args.quick:
        equil, prod = 500, 1000
    if args.high_precision:
        equil, prod = 20000, 100000

    out_dir = Path(args.outdir)
    out_dir.mkdir(parents=True, exist_ok=True)
    logger = ExperimentLogger("tnn_gen", output_dir=str(out_dir)).logger

    logger.info(
        f"L={args.L_list}, T=[{args.T_min},{args.T_max}], n_T={args.n_T}, algo={args.algo}"
    )
    t0 = time.time()
    results = across_L(
        L_list=args.L_list,
        T_min=args.T_min,
        T_max=args.T_max,
        num_replicas=args.n_T,
        equilibration=equil,
        production=prod,
        algorithm=args.algo,
        exchange_interval=5,
        thin=thin,
        n_processes_per_L=1,
        checkpoint_dir=str(out_dir / "ckpt"),
        checkpoint_final=True,
    )
    logger.info(f"æ¨¡æ‹Ÿå®Œæˆï¼Œç”¨æ—¶ {time.time()-t0:.1f}s")

    export_tnn_data(results, out_dir / "npz")
    try:
        plot_overview(results, str(out_dir / "overview.png"))
    except Exception as e:  # noqa: BLE001
        logger.error(f"ç»˜å›¾å¤±è´¥: {e}")

    import pickle
    with open(out_dir / "raw_results.pkl", "wb") as f:
        pickle.dump(results, f)


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š

````
=============
2025-12-02 16:55:12 | INFO | å®éªŒå¼€å§‹: tnn_gen
2025-12-02 16:55:12 | INFO | æ—¶é—´: 2025-12-02T16:55:12.001576
2025-12-02 16:55:12 | INFO | ======================================================================
2025-12-02 16:55:12 | INFO | L=[16, 32, 64], T=[2.0,2.6], n_T=32, algo=wolff
[worker pid=41377] Starting L=32  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=41376] Starting L=16  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=41375] Starting L=64  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=41376] L=16 å·²ä¿å­˜ checkpoint -> remc_L16_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=41376] L=16 completed
[worker pid=41377] L=32 å·²ä¿å­˜ checkpoint -> remc_L32_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=41377] L=32 completed
[worker pid=41375] L=64 å·²ä¿å­˜ checkpoint -> remc_L64_T2.000000-2.600000_R32_h0.000000_wolff_geom.ckpt.json
[worker pid=41375] L=64 completed
2025-12-02 16:57:02 | INFO | æ¨¡æ‹Ÿå®Œæˆï¼Œç”¨æ—¶ 110.1s
âœ“ å¯¼å‡º L=16: data_tnn/npz/tnn_L16.npz
âœ“ å¯¼å‡º L=32: data_tnn/npz/tnn_L32.npz
âœ“ å¯¼å‡º L=64: data_tnn/npz/tnn_L64.npz
ğŸ“Š æ¦‚è§ˆå›¾å·²ä¿å­˜: data_tnn/overview.png
````
### 3.4 æ£€æŸ¥å•ä¸ª tnn_L*.npz æ–‡ä»¶çš„å†…å®¹ï¼Œå¹¶ç”»å‡ºç®€å•æ›²çº¿ã€‚
```python
# examples/analysis/check_tnn_file.py
"""
æ£€æŸ¥å•ä¸ª tnn_L*.npz æ–‡ä»¶çš„å†…å®¹ï¼Œå¹¶ç”»å‡ºç®€å•æ›²çº¿ã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("npz", help="tnn_L*.npz file")
    args = parser.parse_args()

    path = Path(args.npz)
    data = np.load(path)
    print("keys:", list(data.keys()))
    print("L =", data["L"])
    print("temperatures shape:", data["temperatures"].shape)

    T = data["temperatures"]
    E, M = data["E"], data["M"]

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.plot(T, E, "o-")
    plt.xlabel("T")
    plt.ylabel("E")
    plt.subplot(1, 2, 2)
    plt.plot(T, M, "o-")
    plt.xlabel("T")
    plt.ylabel("M")
    plt.tight_layout()
    out_png = path.with_suffix(".check.png")
    plt.savefig(out_png, dpi=200)
    print("Saved preview to", out_png)


if __name__ == "__main__":
    main()

```
ç”¨æ³•ï¼š
````
python check_tnn_file.py /Users//Python/ising-fss/examples/data_tnn/npz/tnn_L16.npz
````

è¾“å‡ºï¼š
````
keys: ['temperatures', 'L', 'E', 'M', 'C', 'chi', 'U', 'E_err', 'M_err', 'C_err', 'chi_err']
L = 16
temperatures shape: (32,)
Saved preview to /Users//Python/ising-fss/examples/data_tnn/npz/tnn_L16.check.png
````

### 3.5 .h5 to train_data
```python
# examples/ml/generate_dl_data.py
"""
ä» REMC HDF5 è¾“å‡ºç”Ÿæˆé€‚åˆ PyTorch çš„è®­ç»ƒé›†æ ¼å¼ã€‚

å‡å®šè¾“å…¥ç›®å½•é‡Œå·²ç»æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª worker å†™å‡ºçš„ .h5 æ–‡ä»¶ï¼Œæˆ–è€… batch_runner merge åçš„ final_ml_data.h5ã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Union

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.data.config_io import load_configs_hdf5, export_for_pytorch


def _find_h5(root: Path) -> Path:
    # ä¼˜å…ˆæ‰¾ final_ml_data.h5ï¼Œå…¶æ¬¡ä»»æ„ .h5
    cand = list(root.rglob("final_ml_data.h5"))
    if cand:
        return cand[0]
    cand = list(root.rglob("*.h5"))
    if not cand:
        raise FileNotFoundError(f"No .h5 found under {root}")
    return cand[0]


def generate_from_hdf5(
    raw_dir: Union[str, Path],
    out_dir: Union[str, Path],
    normalize: bool = True,
    dtype: str = "uint8",
):
    raw_dir = Path(raw_dir)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    h5_path = _find_h5(raw_dir)
    print("Using HDF5:", h5_path)
    ds = load_configs_hdf5(str(h5_path), load_configs=False)

    export_for_pytorch(
        ds,
        save_dir=str(out_dir),
        split_ratio=0.8,
        dtype=dtype,
        normalize=normalize,
        verbose=True,
    )
    print("PyTorch dataset written to", out_dir)


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--raw_dir", required=True, help="ç›®å½•ï¼Œé‡Œé¢æœ‰ REMC çš„ HDF5")
    parser.add_argument("--out_dir", required=True, help="è¾“å‡º PyTorch æ•°æ®é›†ç›®å½•")
    parser.add_argument("--no_normalize", action="store_true")
    parser.add_argument("--dtype", default="uint8")
    args = parser.parse_args()

    generate_from_hdf5(
        raw_dir=args.raw_dir,
        out_dir=args.out_dir,
        normalize=not args.no_normalize,
        dtype=args.dtype,
    )


if __name__ == "__main__":
    main()

```
ç”¨æ³•ï¼š
````

````

### 3.6 cpu_remc_fss_pipeline

```python
# examples/cpu_remc_fss_pipeline.py
"""
åŸºäº CPU / HybridREMCSimulator çš„ REMC â†’ FSS ç®¡çº¿è„šæœ¬ï¼ˆæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œä¸åŒ Lï¼‰ã€‚

åŠŸèƒ½ï¼š
- è¡Œä¸ºå°½é‡æ¨¡ä»¿ gpu_large_scale_fss.pyï¼š
  * æ”¯æŒå¤šæ¬¡è¿è¡ŒåŒä¸€ä¸ª outdirï¼Œè‡ªåŠ¨åœ¨ raw_results.json é‡Œâ€œè¿½åŠ æ ·æœ¬â€ï¼›
  * æ¯æ¬¡ run ä¹‹åéƒ½ç”¨ FSSAnalyzer åšä¸€æ¬¡ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©åˆ†æï¼›
  * æŠŠ Binder U çš„ crossing ä¿¡æ¯å†™å…¥ Tc_est.jsonã€‚
- åŒºåˆ«ï¼š
  * è¿™é‡Œç”¨çš„æ˜¯ HybridREMCSimulatorï¼ˆCPU / æ··åˆå®ç°ï¼‰ï¼Œè€Œä¸æ˜¯ GPU ç‰ˆæ¨¡æ‹Ÿå™¨ï¼›
  * æ”¯æŒé€šè¿‡ --nworkers å¹¶è¡Œè·‘å¤šä¸ª Lï¼ˆæ¯ä¸ª L ä¸€ä¸ª worker è¿›ç¨‹ï¼‰ã€‚
"""

from __future__ import annotations

import sys
import json
import math
from pathlib import Path
from typing import Dict, Any

from multiprocessing import Pool

import numpy as np

# CuPy æ˜¯å¯é€‰çš„ï¼šæ²¡æœ‰ GPU ä¹Ÿä¸ä¼šå½±å“ CPU ç‰ˆè„šæœ¬
try:
    import cupy as cp  # type: ignore
    from cupy import ndarray as cupy_ndarray  # type: ignore
except Exception:
    cp = None
    cupy_ndarray = None

# ---------- sys.path è®¾ç½® ----------
ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.remc_simulator import HybridREMCSimulator
from ising_fss.simulation.dispatcher import make_replica_seeds
from ising_fss.analysis.fss_analyzer import FSSAnalyzer


# ---------- json.dump helper ----------
def json_default(o):
    """
    è®© json.dump èƒ½å¤„ç† numpy / cupy / set ç­‰ç±»å‹ï¼š
      - numpy æ ‡é‡ â†’ Python æ ‡é‡
      - numpy / cupy æ•°ç»„ â†’ list
      - å…¶å®ƒä¸è®¤è¯†çš„ â†’ repr(o)
    """
    # numpy æ ‡é‡
    if isinstance(o, (np.floating, np.integer)):
        return o.item()

    # numpy æ•°ç»„
    if isinstance(o, np.ndarray):
        return o.tolist()

    # cupy æ•°ç»„
    if cp is not None and cupy_ndarray is not None:
        if isinstance(o, cupy_ndarray):  # type: ignore[attr-defined]
            try:
                return cp.asnumpy(o).tolist()  # type: ignore[attr-defined]
            except Exception:
                return repr(o)

    # 0-d array / å…¶å®ƒâ€œæœ‰ item() çš„æ ‡é‡â€
    if hasattr(o, "shape") and getattr(o, "shape", None) == () and hasattr(o, "item"):
        try:
            return o.item()
        except Exception:
            pass

    # set â†’ list
    if isinstance(o, set):
        return list(o)

    # å…œåº•ï¼šå­—ç¬¦ä¸²è¡¨ç¤º
    return repr(o)


# ---------- åŸå§‹ analyze() â†’ FSSAnalyzer è¾“å…¥æ ¼å¼ ----------

def to_fss_format(res_raw: Dict[str, Any]) -> Dict[float, Dict[str, Any]]:
    """
    å°† REMC æ¨¡æ‹Ÿå™¨çš„åŸå§‹ analyze() è¾“å‡ºè½¬æ¢ä¸º FSSAnalyzer éœ€è¦çš„æ ¼å¼ï¼š

        è¾“å…¥ï¼šres_raw = {
            "T_2.100000": {...},
            "T_2.225664": {...},
            "swap": {...},
            "field": 0.0,
            ...
        }

        è¾“å‡ºï¼š{
            2.100000: {...},
            2.225664: {...},
            ...
        }

    åªä¿ç•™ key å½¢å¦‚ "T_..." ä¸” value ä¸º dict çš„æ¡ç›®ã€‚
    å¹¶ä¸”åœ¨è¿™é‡Œå°½é‡æŠŠæ ‡é‡ / æ•°ç»„éƒ½è½¬æˆ float64ï¼Œé¿å…ç²¾åº¦é€€åŒ–ã€‚
    """
    out: Dict[float, Dict[str, Any]] = {}

    for key, val in res_raw.items():
        if not (isinstance(key, str) and key.startswith("T_") and isinstance(val, dict)):
            continue
        try:
            T = float(key.split("_", 1)[1])
        except Exception:
            continue

        obs: Dict[str, Any] = {}
        for k, x in val.items():
            # æ ‡é‡ç±»ï¼šè½¬æˆ numpy.float64ï¼ˆæˆ– Python float ä¹Ÿç­‰ä»·äºåŒç²¾åº¦ï¼‰
            if isinstance(x, (int, float, np.floating)):
                obs[k] = np.float64(x)
            # numpy æ•°ç»„ï¼šè½¬æˆ float64 æ•°ç»„
            elif isinstance(x, np.ndarray):
                obs[k] = np.asarray(x, dtype=np.float64)
            # cupy æ•°ç»„ï¼šå…ˆæ¬åˆ° hostï¼Œå†è½¬ float64
            elif cp is not None and cupy_ndarray is not None and isinstance(x, cupy_ndarray):  # type: ignore[attr-defined]
                obs[k] = cp.asnumpy(x).astype(np.float64)  # type: ignore[attr-defined]
            else:
                # å…¶å®ƒç±»å‹ï¼ˆæ¯”å¦‚å­—ç¬¦ä¸²ã€æ•´æ•°åˆ—è¡¨ã€å…ƒç»„ï¼‰åŸæ ·ä¿ç•™
                obs[k] = x

        out[np.float64(T)] = obs

    return out


# ---------- åˆå¹¶å¤šæ¬¡ runï¼šold + new ----------

def merge_analyze_for_one_L(
    old_L: Dict[str, Any],
    new_L: Dict[str, Any],
    L: int,
) -> Dict[str, Any]:
    """
    æŠŠåŒä¸€ä¸ª Lï¼ˆä¾‹å¦‚ L=128ï¼‰åœ¨å¤šæ¬¡ run ä¸­å¾—åˆ°çš„ analyze() ç»“æœåˆå¹¶ï¼š

    - å¯¹æ¯ä¸ªæ¸©åº¦å— "T_xxx"ï¼š
        * old å’Œ new ä¸­çš„ E_samples / M_samples æ‹¼æ¥ï¼ˆä»¥ float64 å­˜å‚¨ï¼‰ï¼›
        * ç”¨æ‹¼æ¥åçš„åºåˆ—é‡æ–°è®¡ç®—ï¼šE, M, C, chi, U, n_samples ç­‰ï¼›
        * E_err, M_err ç”¨ç®€å• sqrt(var/N) å…œåº•ï¼ˆä¸åš bootstrapï¼‰ï¼Œ
          è¿™æ ·ä¸ GPU/CPU ç‰ˆ analyze() çš„é€»è¾‘ä¿æŒä¸€è‡´çš„é‡çº²ï¼›
    - å¯¹ swapï¼š
        * è‹¥ attempts / accepts ç»´åº¦ä¸€è‡´ï¼Œåˆ™ç›´æ¥é€å¯¹ç›¸åŠ ï¼›
        * å¦åˆ™ä¿ç•™ new_L["swap"]ã€‚
    - å¯¹å…¶å®ƒé”®ï¼ˆfieldã€rng_versions ç­‰ï¼‰ï¼š
        * ä¼˜å…ˆä½¿ç”¨ new_L ä¸­çš„æ¡ç›®ï¼›
        * old_L ä¸­æœ‰è€Œ new_L ä¸­æ²¡æœ‰çš„é”®ä¼šè¢«ä¿ç•™ã€‚
    """
    N_site = int(L) * int(L)
    merged: Dict[str, Any] = {}

    # å…ˆéå†â€œæ–°ç»“æœâ€ï¼Œé€ä¸ª key åˆå¹¶
    for key, new_block in new_L.items():
        # --- æ¸©åº¦å— T_xxx ---
        if isinstance(key, str) and key.startswith("T_") and isinstance(new_block, dict):
            old_block = old_L.get(key, {})

            # æ˜ç¡®ç”¨ float64
            e_old = np.asarray(old_block.get("E_samples", []), dtype=np.float64)
            e_new = np.asarray(new_block.get("E_samples", []), dtype=np.float64)
            m_old = np.asarray(old_block.get("M_samples", []), dtype=np.float64)
            m_new = np.asarray(new_block.get("M_samples", []), dtype=np.float64)

            if e_old.size or e_new.size:
                if e_old.size and e_new.size:
                    e_all = np.concatenate([e_old, e_new])
                else:
                    e_all = e_old if e_old.size else e_new
            else:
                e_all = np.asarray([], dtype=np.float64)

            if m_old.size or m_new.size:
                if m_old.size and m_new.size:
                    m_all = np.concatenate([m_old, m_new])
                else:
                    m_all = m_old if m_old.size else m_new
            else:
                m_all = np.asarray([], dtype=np.float64)

            if e_all.size == 0:
                # æ²¡æœ‰æ ·æœ¬ï¼Œå°±ç›´æ¥ä½¿ç”¨ new_block
                merged[key] = new_block
                continue

            # æ¸©åº¦ T çš„ç¡®å®šä¼˜å…ˆçº§ï¼šnew_block["T"] > old_block["T"] > ä» key è§£æ
            T_val_raw = None
            if isinstance(new_block.get("T", None), (int, float, np.floating)):
                T_val_raw = float(new_block["T"])
            elif isinstance(old_block.get("T", None), (int, float, np.floating)):
                T_val_raw = float(old_block["T"])
            if T_val_raw is None:
                T_val_raw = float(key.split("_", 1)[1])

            T_val = np.float64(T_val_raw)
            beta = np.float64(1.0) / T_val

            mean_e = np.float64(np.mean(e_all))
            if m_all.size:
                mean_m = np.float64(np.mean(m_all))
            else:
                mean_m = np.float64(0.0)

            m2 = m_all ** 2 if m_all.size else np.asarray([], dtype=np.float64)
            m4 = m_all ** 4 if m_all.size else np.asarray([], dtype=np.float64)
            mean_m2 = np.float64(np.mean(m2)) if m2.size else np.float64(0.0)

            var_e = max(np.float64(0.0), np.float64(np.mean(e_all ** 2) - mean_e ** 2))
            if m_all.size:
                var_m = max(np.float64(0.0), mean_m2 - mean_m ** 2)
            else:
                var_m = np.float64(0.0)

            C_point = (beta ** 2) * np.float64(N_site) * var_e
            chi_point = beta * np.float64(N_site) * var_m

            if mean_m2 <= np.float64(1e-15):
                U = np.float64(0.0)
            else:
                m4_mean = np.float64(np.mean(m4)) if m4.size else np.float64(0.0)
                U = np.float64(1.0) - m4_mean / (np.float64(3.0) * (mean_m2 ** 2 + np.float64(1e-16)))

            N_samples = int(e_all.size)
            E_err = np.float64(math.sqrt(float(var_e) / max(1, N_samples)))
            if m_all.size:
                M_err = np.float64(math.sqrt(float(var_m) / max(1, N_samples)))
            else:
                M_err = np.float64(0.0)

            merged[key] = {
                "T": float(T_val),
                "E": float(mean_e),
                "E_err": float(E_err),
                "M": float(mean_m),
                "M_err": float(M_err),
                "C": float(C_point),
                "C_err": 0.0,   # å¦‚éœ€ bootstrapï¼Œå¯åœ¨åå¤„ç†é˜¶æ®µåš
                "chi": float(chi_point),
                "chi_err": 0.0,
                "U": float(U),
                "n_samples": int(N_samples),
                "E_samples": e_all,  # ä¿ç•™ä¸º float64 æ•°ç»„
                "M_samples": m_all,
            }

        # --- swap ç»Ÿè®¡ ---
        elif key == "swap" and isinstance(new_block, dict):
            old_block = old_L.get("swap", {})
            a_old = np.asarray(old_block.get("attempts", []), dtype=np.int64)
            a_new = np.asarray(new_block.get("attempts", []), dtype=np.int64)
            c_old = np.asarray(old_block.get("accepts", []), dtype=np.int64)
            c_new = np.asarray(new_block.get("accepts", []), dtype=np.int64)

            if a_old.size and a_new.size and a_old.size == a_new.size:
                a_all = (a_old + a_new)
                if c_old.size and c_old.size == c_new.size:
                    c_all = (c_old + c_new)
                else:
                    c_all = c_new
                merged[key] = {
                    "attempts": a_all,
                    "accepts": c_all,
                    "total_attempts": int(np.sum(a_all)),
                    "total_accepts": int(np.sum(c_all)),
                }
            else:
                merged[key] = new_block

        # --- å…¶å®ƒé”®ï¼šä¼˜å…ˆ newï¼Œå…¶æ¬¡ old ---
        else:
            if key in old_L and key not in merged:
                merged[key] = old_L[key]
            merged[key] = new_block

    # å†æŠŠ old_L é‡Œé—æ¼çš„é”®è¡¥ä¸Š
    for key, old_block in old_L.items():
        if key not in merged:
            merged[key] = old_block

    return merged


# ---------- CPU ç‰ˆï¼šè·‘å•ä¸ª L çš„ REMC ----------

def run_one_L(L: int, outdir: Path, args) -> Dict[str, Any]:
    """
    è·‘å•ä¸ª L çš„ HybridREMCSimulator REMCï¼Œè¿”å› sim.analyze() çš„åŸå§‹ç»“æœï¼š
        {
          "T_2.100000": {...},
          "T_2.225664": {...},
          "swap": {...},
          "field": 0.0,
          ...
        }
    """
    T_min = float(args.T_min)
    T_max = float(args.T_max)
    num_replicas = int(args.num_replicas)

    replica_seeds = make_replica_seeds(master_seed=10_000 + int(L), n_replicas=num_replicas)

    print(
        f"\n=== è¿è¡Œ REMC (CPU ç‰ˆ): L={L}, "
        f"Tâˆˆ[{T_min}, {T_max}], replicas={num_replicas}, algo=metropolis_sweep ==="
    )

    sim = HybridREMCSimulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        algorithm="metropolis_sweep",
        h=0.0,
        replica_seeds=replica_seeds,
    )

    # æ¯ä¸ª L å•ç‹¬ä¸€ä¸ªå­ç›®å½•ï¼Œç”¨äºä¿å­˜ latticesï¼ˆè‹¥å¯ç”¨ï¼‰
    save_dir_L = outdir / f"L{L}"
    save_dir_L.mkdir(parents=True, exist_ok=True)

    sim.run(
        equilibration_steps=int(args.equil_steps),
        production_steps=int(args.prod_steps),
        exchange_interval=int(args.exchange_interval),
        thin=int(args.thin),
        verbose=bool(args.verbose),
        save_lattices=bool(args.save_lattices),
        save_dir=str(save_dir_L),
        worker_id=f"cpu_L{L}",
        auto_thin=bool(getattr(args, "auto_thin", False)),
        thin_min=int(getattr(args, "thin_min", 1)),
        thin_max=int(getattr(args, "thin_max", 10_000)),
        tau_update_interval=int(getattr(args, "tau_update_interval", 256)),
        tau_window=int(getattr(args, "tau_window", 2048)),
    )

    res = sim.analyze(verbose=False)
    return res


# ---------- ç»™ multiprocessing.Pool ç”¨çš„å°è£… ----------

def _run_one_L_wrapper(args_tuple):
    """
    ç»™ multiprocessing.Pool ç”¨çš„ç®€å•å°è£…ï¼š
        è¾“å…¥: (L, outdir_str, args)
        è¾“å‡º: (L, res_new)
    """
    L, outdir_str, args = args_tuple
    outdir = Path(outdir_str)
    res_new = run_one_L(L, outdir, args)
    return L, res_new


# ---------- å°å·¥å…·ï¼šæŒ‰æ¡ç›®æ¢è¡Œæ‰“å° Tc_est ç»“æœ ----------

def _pretty_print_Tc_est(label: str, est: Dict[str, Any]) -> None:
    print(f"[INFO] {label} ç»“æœ:")

    if not isinstance(est, dict):
        print(f"  {est}")
        return

    for key in ("Tc", "var", "std"):
        if key in est:
            print(f"  {key}: {est[key]}")

    if "weights" in est:
        print("  weights:")
        try:
            for w in est["weights"]:
                print(f"    - {w}")
        except TypeError:
            print(f"    {est['weights']}")

    if "pairs" in est:
        print("  pairs:")
        try:
            for pair in est["pairs"]:
                try:
                    L1, L2 = pair
                    print(f"    - ({L1}, {L2})")
                except Exception:
                    print(f"    - {pair}")
        except TypeError:
            print(f"    {est['pairs']}")

    if "crossings" in est:
        print("  crossings:")
        try:
            for c in est["crossings"]:
                try:
                    L1 = getattr(c, "L1", None)
                    L2 = getattr(c, "L2", None)
                    Tc_c = getattr(c, "Tc", None)
                    slope_diff = getattr(c, "slope_diff", None)
                    bracket = getattr(c, "bracket", None)
                    method = getattr(c, "method", "")
                    note = getattr(c, "note", "")

                    line = "    - "
                    if L1 is not None and L2 is not None:
                        line += f"L1={L1}, L2={L2}, "
                    if Tc_c is not None:
                        try:
                            line += f"Tc={Tc_c:.6f}, "
                        except Exception:
                            line += f"Tc={Tc_c}, "
                    if slope_diff is not None:
                        try:
                            line += f"slope_diff={slope_diff:.3f}, "
                        except Exception:
                            line += f"slope_diff={slope_diff}, "
                    if bracket is not None:
                        line += f"bracket={bracket}, "
                    if method:
                        line += f"method={method}"
                    if note:
                        line += f", note={note}"
                    print(line)
                except Exception:
                    print(f"    - {c}")
        except TypeError:
            print(f"    {est['crossings']}")

    for key, value in est.items():
        if key in ("Tc", "var", "std", "weights", "pairs", "crossings"):
            continue
        print(f"  {key}: {value}")


# ---------- åŸºäº raw_results çš„ FSS åˆ†æ ----------

def run_fss_analysis_from_raw(
    results_all_raw: Dict[str, Dict[str, Any]],
    outdir: Path,
    Tc_theory: float = 2.269185,
) -> Dict[str, Any]:
    """
    ä½¿ç”¨åˆå¹¶åçš„ raw_results åš FSS åˆ†æï¼š
      - å…ˆç”¨ to_fss_format è½¬æˆ FSSAnalyzer è¾“å…¥å½¢å¼ï¼›
      - å†è¡¥å…… *_stderr å­—æ®µï¼›
      - ç„¶åè·‘ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©ã€‚
    è¿”å› estimate_Tc('U') çš„å®Œæ•´å­—å…¸ã€‚
    """
    print("\n=== åŸºäºåˆå¹¶åçš„ raw_results æ„å»º FSSAnalyzer ===")

    results_all_fss: Dict[int, Dict[float, Dict[str, Any]]] = {}
    for L_key, block in results_all_raw.items():
        try:
            L_int = int(L_key)
        except Exception:
            continue

        fss_block = to_fss_format(block)

        # ç»™ FSSAnalyzer è¡¥ä¸Š *_stderr å­—æ®µï¼ˆæ²¿ç”¨ *_errï¼‰
        for obs in fss_block.values():
            if not isinstance(obs, dict):
                continue
            for base in ("E", "M", "C", "chi", "U"):
                err_key = f"{base}_err"
                stderr_key = f"{base}_stderr"
                if err_key in obs and stderr_key not in obs:
                    val = obs[err_key]
                    if isinstance(val, (int, float, np.floating)):
                        obs[stderr_key] = float(val)

        results_all_fss[L_int] = fss_block

    if not results_all_fss:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ FSS æ•°æ®ï¼ˆå¯èƒ½æ‰€æœ‰ L éƒ½ä¸ºç©ºï¼Ÿï¼‰")
        return {}

    analyzer = FSSAnalyzer(results_all_fss, Tc_theory=Tc_theory)

    # 1) Binder U äº¤å‰ â†’ Tc ä¼°è®¡
    Tc_val = None
    Tc_est: Dict[str, Any] = {}
    try:
        est = analyzer.estimate_Tc("U")
        if isinstance(est, dict):
            Tc_est = est
            Tc_val = float(est.get("Tc", None))
            _pretty_print_Tc_est("estimate_Tc('U')", est)
        else:
            Tc_val = float(est)
            Tc_est = {"Tc": Tc_val}
            print(f"[INFO] estimate_Tc('U') å¾—åˆ° Tc â‰ˆ {Tc_val:.6f}")
    except Exception as e:
        print("[WARN] estimate_Tc('U') å¤±è´¥:", e)

    if Tc_val is None:
        Tc_val = Tc_theory
        print(f"[INFO] ä½¿ç”¨ç†è®º Tc = {Tc_val:.6f} ä½œä¸ºåç»­æ‹ŸåˆåŸºå‡†")
    else:
        print(f"[INFO] ä¼°è®¡ Tc â‰ˆ {Tc_val:.6f} (ç†è®ºå€¼ Tcâ‰ˆ{Tc_theory})")

    # 2) ç”¨ Ï‡ çš„ FSS æ‹Ÿåˆ Î³/Î½
    gamma_over_nu = None
    try:
        expo = analyzer.extract_critical_exponents(
            observable="chi",
            Tc_hint=Tc_val,
            fit_nu=False,  # Î½ å·²çŸ¥ä¸º 1 çš„æƒ…å½¢ä¸‹ï¼Œåªæ‹Ÿåˆ Î³/Î½ æ›´ç¨³
        )
        print("exponents (from chi):", expo)

        for k in ["gamma_over_nu", "exponent_ratio", "exponent"]:
            if k in expo:
                gamma_over_nu = float(expo[k])
                print(f"[INFO] è¯†åˆ«åˆ° {k} â‰ˆ {gamma_over_nu:.4f}")
                break
    except TypeError:
        expo = analyzer.extract_critical_exponents("chi")
        print("exponents (from chi):", expo)
        for k in ["gamma_over_nu", "exponent_ratio", "exponent"]:
            if k in expo:
                gamma_over_nu = float(expo[k])
                print(f"[INFO] è¯†åˆ«åˆ° {k} â‰ˆ {gamma_over_nu:.4f}")
                break
    except Exception as e:
        print("[WARN] æå–ä¸´ç•ŒæŒ‡æ•°å¤±è´¥:", e)

    if gamma_over_nu is not None:
        print(
            "[INFO] ç†è®ºå€¼ Î³/Î½ â‰ˆ 1.75; "
            f"å½“å‰æ‹Ÿåˆå¾—åˆ° Î³/Î½ â‰ˆ {gamma_over_nu:.4f}"
        )
        if gamma_over_nu < 0:
            print("[WARN] Î³/Î½ < 0 æ˜æ˜¾è¿èƒŒç‰©ç†å¸¸è¯†ï¼Œè¯´æ˜é‡‡æ ·æˆ–æ‹Ÿåˆè¿˜æœ‰é—®é¢˜ã€‚")
    else:
        print("[WARN] æœªèƒ½ä» expo ä¸­è¯†åˆ«å‡º Î³/Î½ï¼Œåç»­ data collapse å°†ä½¿ç”¨ç†è®ºå€¼ã€‚")
        gamma_over_nu = 1.75

    # 3) åšä¸€æ¬¡ Ï‡ çš„æ•°æ®å¡Œç¼©
    print("\n=== chi æ•°æ®å¡Œç¼© (CPU ç‰ˆ) ===")
    if not hasattr(analyzer, "data_collapse"):
        print("[INFO] å½“å‰ FSSAnalyzer æœªå®ç° data_collapseï¼Œè·³è¿‡è¯¥æ­¥éª¤ã€‚")
    else:
        try:
            collapse = analyzer.data_collapse(
                observable="chi",
                Tc=Tc_val,
                nu=1.0,                # 2D Ising çš„ç†è®º Î½ = 1
                exponent_ratio=gamma_over_nu,
            )
            print("data_collapse keys:", list(collapse.keys()))
            if "score" in collapse:
                print(f"collapse score â‰ˆ {collapse['score']:.6g}")
                print("ï¼ˆscore è¶Šå°é€šå¸¸ä»£è¡¨å¡Œç¼©è´¨é‡è¶Šå¥½ï¼Œä»…ä¾›ç›¸å¯¹æ¯”è¾ƒï¼‰")
        except Exception as e:
            print("[WARN] data_collapse è°ƒç”¨å¤±è´¥:", e)

    # å†™ Tc_est.json
    Tc_path = outdir / "Tc_est.json"
    try:
        with open(Tc_path, "w", encoding="utf-8") as f:
            json.dump(Tc_est, f, indent=2, default=json_default, ensure_ascii=False)
        print(f"âœ… Tc ä¼°è®¡ä¸é…å¯¹ crossing ä¿¡æ¯å·²å†™å…¥ {Tc_path}")
    except Exception as exc:
        print(f"âŒ å†™ Tc_est.json å¤±è´¥: {exc}")

    return Tc_est


# ---------- mainï¼šæ•´ä½“ç®¡çº¿ ----------

def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--L_list", type=int, nargs="+", default=[16, 32, 64],
                        help="è¦è·‘çš„ L åˆ—è¡¨ï¼Œä¾‹å¦‚: --L_list 16 32 64")
    parser.add_argument("--outdir", default="runs/cpu_large_scale_fss",
                        help="è¾“å‡ºç›®å½•ï¼ˆraw_results.json / Tc_est.json / lattices ç­‰ï¼‰")

    # å¹¶è¡Œ worker æ•°ï¼šç”¨äºå¹¶è¡Œè·‘ä¸åŒçš„ L
    parser.add_argument("--nworkers", type=int, default=1,
                        help="å¹¶è¡Œ worker æ•°é‡ï¼Œç”¨äºå¹¶è¡Œè·‘ä¸åŒçš„ Lï¼ˆé»˜è®¤ 1ï¼Œä¸²è¡Œï¼‰ã€‚")

    # ç‰©ç† & æ¨¡æ‹Ÿå‚æ•°ï¼ˆé»˜è®¤å–ä½ åŸæ¥ demo çš„é‚£ä¸€ç»„ï¼‰
    parser.add_argument("--T_min", type=float, default=2.1)
    parser.add_argument("--T_max", type=float, default=2.5)
    parser.add_argument("--num_replicas", type=int, default=16)

    parser.add_argument("--equil_steps", type=int, default=20_000,
                        help="é¢„çƒ­æ­¥æ•°ï¼ˆsweepsï¼‰")
    parser.add_argument("--prod_steps", type=int, default=100_000,
                        help="ç”Ÿäº§é˜¶æ®µæ€» sweeps æ•°ï¼ˆä¸åŒ…å«é¢„çƒ­ï¼‰")
    parser.add_argument("--exchange_interval", type=int, default=5,
                        help="æ¯éš”å¤šå°‘ sweeps å°è¯•ä¸€æ¬¡ replica äº¤æ¢")

    parser.add_argument("--thin", type=int, default=200,
                        help="åˆå§‹ thinning é—´éš”ï¼ˆsweepsï¼‰ã€‚è‹¥ --auto_thinï¼Œåˆ™ä½œä¸ºèµ·å§‹ thinã€‚")

    # è‡ªé€‚åº” thin ç›¸å…³å‚æ•°ï¼ˆHybridREMCSimulator ä¹Ÿæ”¯æŒï¼‰
    parser.add_argument("--auto_thin", action="store_true",
                        help="å¯ç”¨åœ¨çº¿ä¼°è®¡ Ï„_int çš„è‡ªé€‚åº” thinningã€‚")
    parser.add_argument("--thin_min", type=int, default=1,
                        help="è‡ªé€‚åº” thinning çš„æœ€å°å€¼ï¼ˆå•ä½ï¼šsweepsï¼‰ã€‚")
    parser.add_argument("--thin_max", type=int, default=10_000,
                        help="è‡ªé€‚åº” thinning çš„æœ€å¤§å€¼ï¼ˆå•ä½ï¼šsweepsï¼‰ã€‚")
    parser.add_argument("--tau_update_interval", type=int, default=256,
                        help="æ¯éš”å¤šå°‘ä¸ª production sweeps åšä¸€æ¬¡ Ï„_int æ›´æ–°ã€‚")
    parser.add_argument("--tau_window", type=int, default=2048,
                        help="ä¼°è®¡ Ï„_int æ—¶ä½¿ç”¨çš„çª—å£é•¿åº¦ï¼ˆæœ€å¤§å†å²æ ·æœ¬æ•°ï¼‰ã€‚")

    # I/O & å…¶å®ƒ
    parser.add_argument("--save_lattices", action="store_true",
                        help="æ˜¯å¦æŠŠ lattice è½¨è¿¹å†™å…¥ HDF5ï¼ˆæ¯ä¸ªæ¸©åº¦ä¸€ä¸ªæ–‡ä»¶ï¼‰ã€‚")
    parser.add_argument("--verbose", action="store_true",
                        help="æ‰“å°ä¸€äº›è¿›åº¦ä¿¡æ¯ã€‚")

    args = parser.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("CPU REMC â†’ FSSAnalyzer â†’ Tc / Î³/Î½ / æ•°æ®å¡Œç¼©")
    print("=" * 70)
    print(
        f"å‚æ•°æ¦‚è§ˆï¼šL_list={args.L_list}, Tâˆˆ[{args.T_min},{args.T_max}], "
        f"replicas={args.num_replicas}, equil={args.equil_steps}, prod={args.prod_steps}, thin={args.thin}, "
        f"nworkers={args.nworkers}"
    )

    # ---------- è¯»å–æ—§çš„ raw_results.jsonï¼ˆç”¨äºåˆå¹¶æ ·æœ¬ï¼‰ ----------
    raw_path = outdir / "raw_results.json"
    prev_all_raw: Dict[str, Any] = {}
    if raw_path.exists():
        try:
            with open(raw_path, "r", encoding="utf-8") as f:
                prev_all_raw = json.load(f)
            if not isinstance(prev_all_raw, dict):
                prev_all_raw = {}
        except Exception as exc:
            print(f"âš ï¸ è¯»å–å·²æœ‰ raw_results.json å¤±è´¥ï¼Œå°†ä»ç©ºç™½å¼€å§‹: {exc}")
            prev_all_raw = {}
    else:
        prev_all_raw = {}

    # ---------- æœ¬æ¬¡ run çš„ï¼ˆæˆ–åˆå¹¶åçš„ï¼‰ç»“æœ ----------
    results_all_raw: Dict[str, Dict[str, Any]] = {}

    L_list = list(args.L_list)
    tasks = [(int(L), str(outdir), args) for L in L_list]

    if args.nworkers is None or args.nworkers <= 1 or len(L_list) == 1:
        # ä¸²è¡Œæ¨¡å¼ï¼ˆå’Œä»¥å‰è¡Œä¸ºå®Œå…¨ä¸€è‡´ï¼‰
        for L in L_list:
            print(f"\n=== REMC for L={L} ===")
            res_new = run_one_L(int(L), outdir, args)

            L_key = str(L)
            if L_key in prev_all_raw:
                print(f"[L={L}] ğŸ”„ ä¸ raw_results.json ä¸­æ—§æ ·æœ¬è¿›è¡Œåˆå¹¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰")
                merged = merge_analyze_for_one_L(prev_all_raw[L_key], res_new, int(L))
            else:
                merged = res_new

            results_all_raw[L_key] = merged
    else:
        # å¹¶è¡Œæ¨¡å¼ï¼šä¸åŒçš„ L åˆ†é…ç»™ä¸åŒ worker
        print(f"\n=== å¹¶è¡Œæ¨¡å¼ï¼šnworkers={args.nworkers}, L_list={L_list} ===")
        with Pool(processes=args.nworkers) as pool:
            for L, res_new in pool.imap_unordered(_run_one_L_wrapper, tasks):
                print(f"\n=== REMC for L={L} å®Œæˆï¼ˆæ¥è‡ª workerï¼‰ ===")

                L_key = str(L)
                if L_key in prev_all_raw:
                    print(f"[L={L}] ğŸ”„ ä¸ raw_results.json ä¸­æ—§æ ·æœ¬è¿›è¡Œåˆå¹¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰")
                    merged = merge_analyze_for_one_L(prev_all_raw[L_key], res_new, int(L))
                else:
                    merged = res_new

                results_all_raw[L_key] = merged

    # æŠŠè¿™æ¬¡æ²¡æœ‰è·‘åˆ°çš„ Lï¼ˆä½†æ—§ç»“æœé‡Œå­˜åœ¨çš„ï¼‰æ¬è¿‡æ¥
    for L_key, block in prev_all_raw.items():
        if L_key not in results_all_raw:
            results_all_raw[L_key] = block

    # ---------- å†™å›åˆå¹¶åçš„ raw_results.json ----------
    try:
        with open(raw_path, "w", encoding="utf-8") as f:
            json.dump(results_all_raw, f, indent=2, default=json_default, ensure_ascii=False)
        print(f"âœ… åˆå¹¶åçš„ç»Ÿè®¡ç»“æœå·²å†™å…¥ {raw_path}")
    except Exception as exc:
        print(f"âŒ å†™ raw_results.json å¤±è´¥: {exc}")
        return

    # ---------- FSS åˆ†æ ----------
    Tc_est = run_fss_analysis_from_raw(results_all_raw, outdir=outdir)
    print("Done. See", outdir)


if __name__ == "__main__":
    main()

```

ç”¨æ³•ï¼š
````
1ï¼‰ä¸²è¡Œè·‘ï¼ˆè¡Œä¸ºå’Œä¹‹å‰ä¸€æ ·ï¼‰ï¼š

```bash
python examples/pipelines/cpu_remc_large_scale_fss.py \
    --L_list 16 32 64 \
    --T_min 2.1 --T_max 2.5 \
    --num_replicas 16 \
    --equil_steps 20000 \
    --prod_steps 100000 \
    --thin 200 \
    --exchange_interval 5 \
    --outdir runs/cpu_fss_thin200
```

2ï¼‰å¹¶è¡Œè·‘å¤šä¸ª Lï¼ˆæ¯”å¦‚ 3 ä¸ª workerï¼ŒåŒæ­¥è·‘ 64/96/128ï¼‰ï¼š

```bash
python examples/pipelines/cpu_remc_large_scale_fss.py \
    --L_list 64 96 128 \
    --T_min 2.1 --T_max 2.5 \
    --num_replicas 16 \
    --equil_steps 20000 \
    --prod_steps 600000 \
    --thin 200 \
    --exchange_interval 5 \
    --nworkers 3 \
    --outdir runs/cpu_L64_96_128_thin200_parallel
```

ä¸åŠ  `--auto_thin` æ—¶ï¼Œå³ä¸ºâ€œå›ºå®š thinâ€æ¨¡å¼ï¼›
å¦‚æœåé¢ä½ æƒ³è¯•è‡ªé€‚åº” thinï¼Œåªéœ€åœ¨å‘½ä»¤é‡ŒåŠ ä¸Š `--auto_thin` å³å¯ã€‚

````



### 4.1 ä»æ¨¡æ‹Ÿç»“æœåˆ° FSS åˆ†æçš„å…¸å‹å·¥ä½œæµ

1. é€‰å®šè‹¥å¹²ç³»ç»Ÿå°ºå¯¸ï¼š`L = 8, 16, 32, 64, ...`
2. å¯¹æ¯ä¸ª `L`ï¼š

   * åœ¨ä¸´ç•Œç‚¹é™„è¿‘çš„ä¸€æ®µæ¸©åº¦åŒºé—´ `[T_min, T_max]` ä¸Šè¿è¡Œ REMC
   * å°† `analyze()` çš„è¾“å‡ºï¼ˆæ¯ä¸ªæ¸©åº¦ä¸€ä¸ª dict å« C / Ï‡ / U / n_samplesï¼‰ä¿å­˜ä¸º JSON / NPZ
3. åœ¨å•ç‹¬çš„åˆ†æè„šæœ¬ä¸­ï¼š

   * è¯»å–æ‰€æœ‰ L çš„ç»“æœï¼Œæ•´ç†ä¸ºç»“æ„åŒ–æ•°æ®ï¼š

     ```python
     data[L][T]["C"], data[L][T]["chi"], data[L][T]["U"]
     ```
   * å®ç° Binder äº¤å‰ç‚¹æœç´¢ / ä¸´ç•ŒæŒ‡æ•°æ‹Ÿåˆ / æ•°æ®åç¼©ç­‰

ç¤ºæ„ä»£ç ï¼š

```python
# examples/41_publication_run0.py
"""
â€œè®ºæ–‡çº§â€ FSS ç”Ÿäº§è„šæœ¬ï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼š
- å¤šä¸ª L
- è¾ƒé•¿ REMC
- ä¿å­˜ raw ç»“æœ + FSS-friendly ç»“æœ
- ç”¨ FSS-friendly ç»“æœå–‚ç»™ FSSAnalyzer
"""

from __future__ import annotations

import sys
import json
from pathlib import Path
from typing import Dict, Any

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.parallel import across_L
from ising_fss.analysis.fss_analyzer import FSSAnalyzer


# ---------- JSON åºåˆ—åŒ– helper ----------
def json_default(o):
    if isinstance(o, (np.floating, np.integer)):
        return o.item()
    if isinstance(o, np.ndarray):
        return o.tolist()
    if isinstance(o, set):
        return list(o)
    return repr(o)


# ---------- æŠŠ across_L çš„ raw ç»“æœï¼Œè½¬æ¢æˆ FSSAnalyzer æœŸå¾…çš„ç»“æ„ ----------
def to_fss_results(
    raw: Dict[Any, Any]
) -> Dict[int, Dict[float, Dict[str, float]]]:
    """
    è¾“å…¥ï¼šacross_L è¿”å›çš„ raw ç»“æœ
          raw[L] åŸºæœ¬ä¸Šæ˜¯ sim.analyze() çš„å­—å…¸ï¼ŒåŒ…æ‹¬ 'T_2.000000'ã€'swap' ç­‰é”®
    è¾“å‡ºï¼šFSSAnalyzer æœŸæœ›çš„ç»“æ„ï¼š
          { L : { T(float) : { 'E': ..., 'M': ..., 'C': ..., 'chi': ..., 'U': ... } } }
    """
    out: Dict[int, Dict[float, Dict[str, float]]] = {}

    for L_key, res in raw.items():
        # 1) è§£æ L
        try:
            L = int(L_key)
        except Exception:
            if isinstance(L_key, int):
                L = L_key
            else:
                print(f"[WARN] skip non-int L key: {L_key!r}")
                continue

        if not isinstance(res, dict):
            print(f"[WARN] raw[{L}] is not dict, got {type(res)}; skip")
            continue

        temp_map: Dict[float, Dict[str, float]] = {}

        for key, val in res.items():
            # åªä¿ç•™å½¢å¦‚ 'T_2.345000' çš„é”®
            if not (isinstance(key, str) and key.startswith("T_")):
                continue
            try:
                T = float(key.split("_", 1)[1])
            except Exception:
                print(f"[WARN] cannot parse temperature key {key!r} at L={L}")
                continue

            if not isinstance(val, dict):
                # ç†è®ºä¸Šè¿™é‡Œåº”è¯¥æ˜¯ analyze() è¿”å›çš„ per-T dict
                print(f"[WARN] value at L={L}, {key} is not dict ({type(val)}); skip")
                continue

            obs: Dict[str, float] = {}
            for name in ["E", "M", "C", "chi", "U"]:
                if name not in val:
                    continue
                v = val[name]
                # å¦‚æœæ˜¯æ•°ç»„/åˆ—è¡¨ï¼Œå–å‡å€¼
                if isinstance(v, (list, tuple, np.ndarray)):
                    try:
                        v = float(np.mean(v))
                    except Exception:
                        continue
                else:
                    try:
                        v = float(v)
                    except Exception:
                        continue
                obs[name] = v

            if obs:
                temp_map[T] = obs

        if not temp_map:
            print(f"[WARN] no valid temperature entries for L={L}; this size will be empty in FSS.")
        out[L] = temp_map

    return out


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--L_list", type=int, nargs="+", default=[16, 32])
    parser.add_argument("--T_min", type=float, default=2.0)
    parser.add_argument("--T_max", type=float, default=2.6)
    parser.add_argument("--replicas", type=int, default=16)
    parser.add_argument("--equil", type=int, default=5000)
    parser.add_argument("--prod", type=int, default=20000)
    parser.add_argument("--algo", default="metropolis_sweep")
    parser.add_argument("--outdir", default="runs/publication_fss")
    args = parser.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # ---------- 1. å¤§è§„æ¨¡ REMC ----------
    raw_results = across_L(
        L_list=args.L_list,
        T_min=args.T_min,
        T_max=args.T_max,
        num_replicas=args.replicas,
        equilibration=args.equil,
        production=args.prod,
        algorithm=args.algo,
        exchange_interval=5,
        thin=5,
        n_processes_per_L=1,
        checkpoint_dir=str(outdir / "ckpt"),
        checkpoint_final=True,
    )

    # ---------- 2. ä¿å­˜ raw ç»“æœï¼ˆåŸå§‹ analyze è¾“å‡ºï¼‰ ----------
    raw_json = outdir / "raw_results.json"
    with raw_json.open("w", encoding="utf-8") as f:
        json.dump(raw_results, f, indent=2, default=json_default)
    print(f"[INFO] raw results saved to {raw_json}")

    # ---------- 3. è½¬æ¢æˆ FSS-friendly ç»“æ„ ----------
    fss_results = to_fss_results(raw_results)

    fss_json = outdir / "fss_results.json"
    with fss_json.open("w", encoding="utf-8") as f:
        json.dump(fss_results, f, indent=2, default=json_default)
    print(f"[INFO] FSS-friendly results saved to {fss_json}")

    # ---------- 4. FSS åˆ†æ ----------
    analyzer = FSSAnalyzer(fss_results)

    # (1) Tc ä¼°è®¡
    try:
        Tc_est = analyzer.estimate_Tc("U")
        Tc_json = outdir / "Tc_est.json"
        with Tc_json.open("w", encoding="utf-8") as f:
            json.dump(Tc_est, f, indent=2, default=json_default)
        print(f"[INFO] Tc estimate saved to {Tc_json}")
    except Exception as e:
        print("[WARN] estimate_Tc('U') failed:", e)

    # (2) ä¸´ç•ŒæŒ‡æ•°ç¤ºä¾‹ï¼ˆchiï¼‰
    try:
        expo = analyzer.extract_critical_exponents("chi")
        expo_json = outdir / "exponents_chi.json"
        with expo_json.open("w", encoding="utf-8") as f:
            json.dump(expo, f, indent=2, default=json_default)
        print(f"[INFO] critical exponents (chi) saved to {expo_json}")
    except Exception as e:
        print("[WARN] extract_critical_exponents('chi') failed:", e)

    print("Publication run finished. Results under", outdir)


if __name__ == "__main__":
    main()


```
è¾“å‡ºï¼š
````
[worker pid=42956] Starting L=16  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=42957] Starting L=32  seed=None replica_seeds_provided=True  h=0.0  checkpoint=ON
[worker pid=42956] L=16 å·²ä¿å­˜ checkpoint -> remc_L16_T2.000000-2.600000_R16_h0.000000_metropolis_sweep_geom.ckpt.json
[worker pid=42957] L=32 å·²ä¿å­˜ checkpoint -> remc_L32_T2.000000-2.600000_R16_h0.000000_metropolis_sweep_geom.ckpt.json
[worker pid=42956] L=16 completed
[worker pid=42957] L=32 completed
[INFO] raw results saved to runs/publication_fss/raw_results.json
[INFO] FSS-friendly results saved to runs/publication_fss/fss_results.json
[INFO] Tc estimate saved to runs/publication_fss/Tc_est.json
[FSSAnalyzer] extract_critical_exponents: insufficient per-point stderr for weighted fit; using unweighted LS.
[INFO] critical exponents (chi) saved to runs/publication_fss/exponents_chi.json
Publication run finished. Results under runs/publication_fss
````

### 4.2 ä½¿ç”¨ GPU REMC å¯¹å¤§ L ç³»ç»Ÿåš FSS çš„éª¨æ¶ç¤ºä¾‹ã€‚
```python
# examples/pipelines/gpu_large_scale_fss.py
"""
ä½¿ç”¨ GPU REMC å¯¹å¤§ L ç³»ç»Ÿåš FSS çš„éª¨æ¶ç¤ºä¾‹ï¼ˆå¸¦ auto_thin ä¸ checkpointï¼‰ã€‚

åŠŸèƒ½æ¦‚è§ˆï¼š
- å¯¹å¤šä¸ª L è¿è¡Œ GPU REMC æ¨¡æ‹Ÿï¼›
- æ¯ä¸ª L å•ç‹¬å»ºç›®å½•ï¼ˆä¾‹å¦‚ runs/gpu_large_scale_fss/L64/ï¼‰ï¼›
- æ¯ä¸ª L å®Œæˆåè‡ªåŠ¨ä¿å­˜ checkpointï¼ˆJSON + NPZï¼‰ï¼›
- å¯é€šè¿‡ --resume ä»ç°æœ‰ checkpoint ç»­è·‘ï¼›
- æ”¯æŒåœ¨å‘½ä»¤è¡Œæ‰“å¼€ auto_thinï¼ˆç”± GPU_REMC_Simulator.run å®ç°ï¼‰ï¼›
- è¾“å‡ºï¼š
    - raw_results.json      : æ¯ä¸ª L çš„åŸå§‹ analyze() ç»“æœ
    - Tc_est.json           : FSSAnalyzer çš„ä¸´ç•Œæ¸©åº¦ä¼°è®¡
"""

from __future__ import annotations

import sys
import json
from pathlib import Path
from typing import Dict, Any

import numpy as np

# CuPy æ˜¯å¯é€‰çš„ï¼šæ²¡æœ‰ GPU ä¹Ÿä¸è‡³äº import å´©æ‰
try:
    import cupy as cp  # type: ignore
except Exception:
    cp = None

# æŠŠé¡¹ç›®æ ¹ç›®å½•å’Œ src åŠ è¿› sys.path
ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.simulation.dispatcher import make_replica_seeds, gpu_available
from ising_fss.simulation.gpu_remc_simulator import GPU_REMC_Simulator  # noqa: E402
from ising_fss.analysis.fss_analyzer import FSSAnalyzer


# ---------- JSON åºåˆ—åŒ– helper ----------
def json_default(o):
    """
    è®© json.dump èƒ½å¤„ç† numpy / cupy / set ç­‰ç±»å‹ï¼š
      - numpy æ ‡é‡ â†’ Python æ ‡é‡
      - numpy / cupy æ•°ç»„ â†’ list
      - å…¶å®ƒä¸è®¤è¯†çš„ â†’ repr(o)
    """
    # numpy æ ‡é‡
    if isinstance(o, (np.floating, np.integer)):
        return o.item()

    # numpy æ•°ç»„
    if isinstance(o, np.ndarray):
        return o.tolist()

    # cupy æ•°ç»„
    if cp is not None:
        try:
            import cupy as _cp  # type: ignore
            if isinstance(o, _cp.ndarray):  # type: ignore[attr-defined]
                return _cp.asnumpy(o).tolist()
        except Exception:
            pass

    # 0-d array / å…¶å®ƒâ€œæœ‰ item() çš„æ ‡é‡â€
    if hasattr(o, "shape") and getattr(o, "shape", None) == () and hasattr(o, "item"):
        try:
            return o.item()
        except Exception:
            pass

    # set â†’ list
    if isinstance(o, set):
        return list(o)

    # å…œåº•ï¼šå­—ç¬¦ä¸²è¡¨ç¤º
    return repr(o)


# ---------- å°† GPU åŸå§‹ç»“æœè½¬ä¸º FSSAnalyzer éœ€è¦çš„ç»“æ„ ----------
def to_fss_format(res_raw: Dict[str, Any]) -> Dict[float, Dict[str, Any]]:
    """
    å°† GPU æ¨¡æ‹Ÿå™¨çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸º FSSAnalyzer éœ€è¦çš„æ ¼å¼ï¼š

        è¾“å…¥ï¼šres_raw = {
            "T_2.100000": {...},
            "T_2.225664": {...},
            "swap": {...},
            "field": 0.0,
            ...
        }

        è¾“å‡ºï¼š{
            2.100000: {...},
            2.225664: {...},
            ...
        }

    åªä¿ç•™ key å½¢å¦‚ "T_..." ä¸” value ä¸º dict çš„æ¡ç›®ã€‚
    """
    out: Dict[float, Dict[str, Any]] = {}
    for key, val in res_raw.items():
        if not isinstance(key, str):
            continue
        if not key.startswith("T_"):
            # è·³è¿‡ 'swap', 'field', 'rng_model' ç­‰éæ¸©åº¦é”®
            continue
        if not isinstance(val, dict):
            continue
        try:
            T = float(key.split("_", 1)[1])
        except Exception:
            continue
        out[T] = val
    return out


# ---------- å•ä¸ª L çš„æ¨¡æ‹Ÿï¼ˆæ”¯æŒ resume + checkpointï¼‰ ----------
def run_one_L(L: int, args, outdir: Path) -> Dict[str, Any]:
    """
    è·‘å•ä¸ª L çš„ GPU REMCï¼Œè¿”å› GPU æ¨¡æ‹Ÿå™¨çš„åŸå§‹ analyze() ç»“æœï¼š
        {
          "T_2.100000": {...},
          "T_2.225664": {...},
          "swap": {...},
          "field": 0.0,
          ...
        }

    - æ¯ä¸ª L ç‹¬ç«‹å­ç›®å½•ï¼š outdir / f"L{L}"
    - checkpoint æ–‡ä»¶ï¼š   outdir / f"L{L}/gpu_L{L}_ckpt.json"
    """
    L_dir = outdir / f"L{L}"
    L_dir.mkdir(parents=True, exist_ok=True)

    ckpt_path = L_dir / f"gpu_L{L}_ckpt.json"

    # æ¸©åº¦èŒƒå›´å’Œå‰¯æœ¬æ•°ä» args ä¸­å–ï¼Œä¿æŒçµæ´»
    T_min = float(args.T_min)
    T_max = float(args.T_max)
    num_replicas = int(args.num_replicas)

    # æ˜¾å¼ replica_seedsï¼Œç¡®ä¿å¯å¤ç°
    replica_seeds = make_replica_seeds(
        master_seed=L * 10,
        n_replicas=num_replicas,
    )

    # æ„é€  GPU æ¨¡æ‹Ÿå™¨å®ä¾‹
    sim = GPU_REMC_Simulator(
        L=L,
        T_min=T_min,
        T_max=T_max,
        num_replicas=num_replicas,
        algorithm="metropolis",  # å†…éƒ¨ä¼š normalize æˆ metropolis_sweep
        h=0.0,
        replica_seeds=replica_seeds,
    )

    # ------- æ˜¯å¦ä» checkpoint ç»­è·‘ -------
    equil_steps = int(args.equil_steps)
    prod_steps = int(args.prod_steps)

    if args.resume and ckpt_path.exists():
        print(f"[L={L}] ğŸ” ä» checkpoint æ¢å¤ï¼š{ckpt_path}")
        try:
            info = sim.restore_from_checkpoint(str(ckpt_path))
            print(f"[L={L}] restore info: {info}")
        except Exception as exc:
            print(f"[L={L}] æ¢å¤å¤±è´¥ï¼Œå°†ä»å¤´è·‘ä¸€éï¼š{exc}")
        else:
            # ç»­è·‘æ—¶é€šå¸¸ä¸å†åšé¢å¤–çƒ­åŒ–
            equil_steps = 0

    # ------- æ­£å¼è¿è¡Œ -------
    print(
        f"[L={L}] è¿è¡Œå‚æ•°: Tâˆˆ[{T_min}, {T_max}], replicas={num_replicas}, "
        f"equil_steps={equil_steps}, prod_steps={prod_steps}, "
        f"exchange_interval={args.exchange_interval}, thin={args.thin}, "
        f"auto_thin={args.auto_thin}"
    )

    sim.run(
        equilibration_steps=equil_steps,
        production_steps=prod_steps,
        exchange_interval=int(args.exchange_interval),
        thin=int(args.thin),
        verbose=args.verbose,
        save_lattices=args.save_lattices,
        save_dir=str(L_dir),
        worker_id=f"gpu_L{L}",
        auto_thin=bool(args.auto_thin),
        thin_min=int(args.thin_min),
        thin_max=int(args.thin_max),
        tau_update_interval=args.tau_update_interval,
        tau_window=int(args.tau_window),
        unit_sanity_check=True,
    )

    # ------- è¿è¡Œç»“æŸåç«‹å³å†™ checkpoint æ–¹ä¾¿ç»­è·‘ -------
    try:
        sim.save_checkpoint(str(ckpt_path))
        print(f"[L={L}] âœ… checkpoint å·²ä¿å­˜åˆ° {ckpt_path}")
    except Exception as exc:
        print(f"[L={L}] âš ï¸ ä¿å­˜ checkpoint å¤±è´¥ï¼š{exc}")

    # ------- è¿”å›åŸå§‹åˆ†æç»“æœ -------
    res_raw = sim.analyze(verbose=False)
    return res_raw


# ---------- ä¸»ç¨‹åº ----------
def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="GPU REMC + FSS pipelineï¼ˆæ”¯æŒ auto_thin ä¸ checkpointï¼‰"
    )
    parser.add_argument("--L_list", type=int, nargs="+", default=[64, 96, 128],
                        help="ç³»ç»Ÿå°ºå¯¸åˆ—è¡¨ï¼Œä¾‹å¦‚: --L_list 64 96 128")
    parser.add_argument("--outdir", default="runs/gpu_large_scale_fss",
                        help="è¾“å‡ºç›®å½•ï¼ˆå°†è‡ªåŠ¨åˆ›å»ºå­ç›®å½• L{L}ï¼‰")

    # ç‰©ç†/æ¨¡æ‹Ÿå‚æ•°
    parser.add_argument("--T_min", type=float, default=2.1,
                        help="æ¸©åº¦ä¸‹é™")
    parser.add_argument("--T_max", type=float, default=2.5,
                        help="æ¸©åº¦ä¸Šé™")
    parser.add_argument("--num_replicas", type=int, default=64,
                        help="å‰¯æœ¬æ•°ï¼ˆæ¸©åº¦æ§½æ•°é‡ï¼‰")

    parser.add_argument("--equil_steps", type=int, default=20000,
                        help="çƒ­åŒ–æ­¥æ•°ï¼ˆsweepsï¼‰")
    parser.add_argument("--prod_steps", type=int, default=100000,
                        help="é‡‡æ ·ç”Ÿäº§æ­¥æ•°ï¼ˆsweepsï¼‰")
    parser.add_argument("--exchange_interval", type=int, default=10,
                        help="æ¯å¤šå°‘ sweeps åšä¸€æ¬¡ replica äº¤æ¢")
    parser.add_argument("--thin", type=int, default=50,
                        help="åˆå§‹é‡‡æ ·é—´éš” thinï¼ˆauto_thin å…³é—­æ—¶å›ºå®šä½¿ç”¨ï¼‰")

    # auto_thin é…ç½®ï¼ˆç”± GPU_REMC_Simulator.run å®ç°ï¼‰
    parser.add_argument("--auto_thin", action="store_true",
                        help="å¼€å¯ GPU ç«¯è‡ªé€‚åº” thinningï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--thin_min", type=int, default=1,
                        help="auto_thin æ—¶æœ€å° thin")
    parser.add_argument("--thin_max", type=int, default=10000,
                        help="auto_thin æ—¶æœ€å¤§ thin")
    parser.add_argument("--tau_update_interval", type=int, default=256,
                        help="auto_thin: å¤šä¹…æ›´æ–°ä¸€æ¬¡ Ï„_int (ä»¥ sweep è®¡)")
    parser.add_argument("--tau_window", type=int, default=2048,
                        help="auto_thin: ä¼°è®¡ Ï„_int æ—¶ä½¿ç”¨çš„çª—å£é•¿åº¦")

    # å…¶å®ƒæ§åˆ¶é¡¹
    parser.add_argument("--save_lattices", action="store_true",
                        help="æ˜¯å¦æŠŠæ ¼ç‚¹å¿«ç…§å†™å…¥ HDF5ï¼ˆæ¯ä¸ªæ¸©åº¦ä¸€ä¸ªæ–‡ä»¶ï¼‰")
    parser.add_argument("--resume", action="store_true",
                        help="è‹¥å­˜åœ¨ checkpointï¼Œåˆ™ä» checkpoint ç»­è·‘ï¼ˆçƒ­åŒ–æ­¥æ•°è‡ªåŠ¨ç½® 0ï¼‰")
    parser.add_argument("--verbose", action="store_true",
                        help="æ‰“å°ä¸€äº›è¿›åº¦ä¿¡æ¯")

    args = parser.parse_args()

    if not gpu_available():
        print("âŒ GPU ä¸å¯ç”¨ï¼Œæœ¬ç¤ºä¾‹æ— æ³•è¿è¡Œã€‚")
        return

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜â€œåŸå§‹ GPU è¾“å‡ºâ€å’Œâ€œä¾› FSS ä½¿ç”¨çš„æ•´å½¢ç»“æœâ€å„ä¸€ä»½
    results_all_raw: Dict[int, Dict[str, Any]] = {}
    results_all_fss: Dict[int, Dict[float, Dict[str, Any]]] = {}

    for L in args.L_list:
        print(f"\n=== ğŸš€ GPU REMC for L={L} ===")
        res_raw = run_one_L(L, args, outdir)
        results_all_raw[L] = res_raw
        results_all_fss[L] = to_fss_format(res_raw)

    # æ³¨æ„ï¼šresults_all_raw é‡Œä¼šåŒ…å« numpy/cupy æ•°ç»„ï¼Œå¿…é¡»ç”¨ json_default
    raw_path = outdir / "raw_results.json"
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(results_all_raw, f, indent=2, default=json_default, ensure_ascii=False)
    print(f"\n[âœ“] åŸå§‹ GPU ç»“æœå·²å†™å…¥: {raw_path}")

    # æŠŠâ€œæ•´å½¢åâ€çš„ results_all_fss å–‚ç»™ FSSAnalyzer
    analyzer = FSSAnalyzer(results_all_fss)
    Tc_est = analyzer.estimate_Tc("U")

    tc_path = outdir / "Tc_est.json"
    with open(tc_path, "w", encoding="utf-8") as f:
        json.dump(Tc_est, f, indent=2, default=json_default, ensure_ascii=False)
    print(f"[âœ“] Tc ä¼°è®¡å·²å†™å…¥: {tc_path}")

    print("\nDone. See", outdir)


if __name__ == "__main__":
    main()

```
è¾“å‡ºï¼š
````

````

### 4.3 å¤šæ¸©åº¦ç‹¬ç«‹ Metropolis é‡‡æ ·ï¼ˆé REMCï¼‰ï¼Œç”¨äºç”Ÿæˆ ML æ•°æ®ã€‚
```python
# examples/pipelines/run_path_A_independent.py
"""
è·¯å¾„ Aï¼šå¤šæ¸©åº¦ç‹¬ç«‹ Metropolis é‡‡æ ·ï¼ˆé REMCï¼‰ï¼Œç”¨äºç”Ÿæˆ ML æ•°æ®ã€‚
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import List

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
for p in (ROOT, ROOT / "src"):
    s = str(p)
    if s not in sys.path:
        sys.path.insert(0, s)

from ising_fss.core.algorithms import update_batch, spawn_replica_seeds  # ä½ å·²æœ‰çš„æ¥å£
from ising_fss.data.data_manager import save_ml_dataset  # å‡è®¾æœ‰ç±»ä¼¼å‡½æ•°


def simulate_independent(
    L: int,
    temps: List[float],
    n_configs_per_T: int,
    n_sweeps_per_sample: int,
    out_h5: Path,
):
    R = len(temps)
    spins_batch = np.random.choice([-1, 1], size=(R, L, L)).astype(np.int8)
    seeds = spawn_replica_seeds(master_seed=1234, n_replicas=R)

    records = []
    for i in range(n_configs_per_T):
        update_batch(
            spins_batch=spins_batch,
            beta=[1.0 / T for T in temps],
            replica_seeds=seeds,
            algo="metropolis_sweep",
            h=0.0,
            n_sweeps=n_sweeps_per_sample,
        )
        records.append(spins_batch.copy())

    configs = np.stack(records, axis=0)  # (n_configs, R, L, L)
    save_ml_dataset(configs=configs, temps=temps, out_path=str(out_h5))


def main():
    L = 32
    temps = np.linspace(1.6, 3.2, 40).tolist()
    simulate_independent(
        L=L,
        temps=temps,
        n_configs_per_T=1000,
        n_sweeps_per_sample=10,
        out_h5=Path("runs/pathA_independent_L32.h5"),
    )


if __name__ == "__main__":
    main()

```
ä½¿ç”¨æ–¹æ³•ï¼š


```bash
# æœ€ç®€å•è·‘ä¸€éï¼ˆå›ºå®š thin=50ï¼Œä¸å¼€ auto_thinï¼Œå¸¦ checkpointï¼‰
python gpu_large_scale_fss.py --L_list 64 96 128 --save_lattices

# æƒ³è¦å¼€å¯ auto_thinï¼š
python gpu_large_scale_fss.py --L_list 64 96 128 --save_lattices --auto_thin --resume

# æ¨¡æ‹Ÿé€”ä¸­ä¸­æ–­åæƒ³ä» checkpoint ç»­è·‘ï¼ˆå†åŠ  50000 ä¸ª production sweepsï¼‰
python gpu_large_scale_fss.py --L_list 64 96 128 --resume --prod_steps 50000

python gpu_large_scale_fss.py \
    --L_list 64 96 128 \
    --auto_thin \
    --prod_steps 100000 \
    --save_lattices \
    --resume

python 42_gpu_large_scale_fss.py     --L_list 64 96 128     --auto_thin   --prod_steps 400000     --save_lattices     --resume

python gpu_large_scale_fss.py \
    --L_list 64 96 128 \
    --equil_steps 20000 \
    --prod_steps 100000 \
    --thin 50 \
    --auto_thin \
    --save_lattices \
    --resume

python gpu_large_scale_fss.py \
    --L_list 64 96 128 \
    --equil_steps 20000 \
    --prod_steps 100000 \
    --thin 50 \
    --auto_thin \
    --save_lattices
    --resume


python 42_gpu_large_scale_fss.py     --L_list 16     --num_replicas 16     --T_min 2.0 --T_max 3.0     --equil_steps 500     --prod_steps 1000     --thin 10     --exchange_interval 5     --outdir runs/test_mini     --verbose     --save_lattices --resume    


````


> ä¹Ÿå¯ä»¥åŸºäºè¿™äº›æ•°æ®å®ç°æ›´ç³»ç»Ÿçš„ `FSSAnalyzer` ç±»ï¼Œå°è£…åœ¨ `ising_fss.analysis` ä¸­ã€‚

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
ising-fss/
â”œâ”€â”€ src/ising_fss/
â”‚Â  Â â”œâ”€â”€ core/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ ¸å¿ƒç®—æ³•ï¼ˆCPU/GPUï¼‰
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ algorithms.pyÂ  Â  Â  Â  # Metropolis/Wolff/SWï¼ˆCPU + Numba JITï¼‰
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ gpu_algorithms.pyÂ  Â  # GPU åŠ é€Ÿï¼ˆCuPyï¼‰
â”‚Â  Â â”‚Â  Â â””â”€â”€ observables.pyÂ  Â  Â  Â # ç‰©ç†é‡è®¡ç®—ï¼ˆèƒ½é‡ã€ç£åŒ–ç­‰ï¼‰
â”‚Â  Â â”œâ”€â”€ simulation/Â  Â  Â  Â  Â  Â  Â  # æ¨¡æ‹Ÿå™¨ä¸è°ƒåº¦
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ remc_simulator.pyÂ  Â  # CPU REMC æ¨¡æ‹Ÿå™¨ï¼ˆSlot-bound RNGï¼‰
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ gpu_remc_simulator.py# GPU REMC æ¨¡æ‹Ÿå™¨
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ dispatcher.pyÂ  Â  Â  Â  # åç«¯ç»Ÿä¸€è°ƒåº¦ï¼ˆCPU/GPU/Autoï¼‰
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ parallel.pyÂ  Â  Â  Â  Â  # è·¨æ™¶æ ¼å°ºå¯¸å¹¶è¡Œä»»åŠ¡ï¼ˆspawn-safeï¼‰
â”‚Â  Â â”‚Â  Â â””â”€â”€ batch_runner.pyÂ  Â  Â  # åˆ†å¸ƒå¼ç”Ÿäº§ä»»åŠ¡å¯åŠ¨å™¨
â”‚Â  Â â”œâ”€â”€ analysis/Â  Â  Â  Â  Â  Â  Â  Â  # ç»Ÿè®¡åˆ†æä¸ FSS
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ fss_analyzer.pyÂ  Â  Â  # FSS ä¸»åˆ†æå™¨ï¼ˆTc/æŒ‡æ•°/åç¼©ï¼‰
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ statistics.pyÂ  Â  Â  Â  # æ—¶é—´åºåˆ—è¯¯å·®åˆ†æï¼ˆÏ„_int/Bootstrapï¼‰
â”‚Â  Â â”‚Â  Â â””â”€â”€ dl_tools.pyÂ  Â  Â  Â  Â  # PyTorch æ•°æ®å·¥å…·
â”‚Â  Â â”œâ”€â”€ data/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ•°æ®ç®¡ç†
â”‚Â  Â â”‚Â  Â â”œâ”€â”€ data_manager.pyÂ  Â  Â  # æµå¼åˆå¹¶ + åŸå­åŒ– I/O
â”‚Â  Â â”‚Â  Â â””â”€â”€ config.pyÂ  Â  Â  Â  Â  Â  # é…ç½®ç®¡ç†ï¼ˆé¢„è®¾/éªŒè¯/CLIï¼‰
â”‚Â  Â â””â”€â”€ utils/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # å·¥å…·å‡½æ•°
â”‚Â  Â  Â  Â â”œâ”€â”€ logger.py
â”‚Â  Â  Â  Â â””â”€â”€ config.py
â”œâ”€â”€ tests/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # å•å…ƒæµ‹è¯•ï¼ˆpytestï¼‰
â”œâ”€â”€ examples/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Jupyter ç¤ºä¾‹
â””â”€â”€ docs/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Sphinx æ–‡æ¡£
```


> å®é™…ç›®å½•å¯èƒ½éšå¼€å‘æ¼”è¿›ç•¥æœ‰è°ƒæ•´ï¼Œè¯·ä»¥ä»“åº“å½“å‰ç»“æ„ä¸ºå‡†ã€‚

---

## ğŸ”¬ æ ¸å¿ƒæ¨¡å—ç®€è¦è¯´æ˜

### `core.algorithms`

* æä¾›å„ç§æ›´æ–°ç®—æ³•çš„ç»Ÿä¸€æ¥å£ï¼š

  * `get_algorithm(name: str)` â†’ è¿”å›å¯¹åº”çš„æ›´æ–°å‡½æ•°
  * æ›´æ–°å‡½æ•°ç­¾åçº¦å®šï¼š

    ```python
    def algo(lattice: np.ndarray, beta: float, rng: np.random.Generator, h: float):
        """
        è¿”å›:
            lattice_out: np.ndarray  # æ›´æ–°åçš„æ ¼ç‚¹ï¼Œè‡ªæ—‹ âˆˆ {-1, +1}
            meta: dict               # é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ç°‡å¤§å°, rng_consumed ç­‰ï¼‰
        """
    ```
* Metropolisï¼š

  * æ£‹ç›˜æ ¼åˆ†è§£ï¼ˆçº¢é»‘æ›´æ–°ï¼‰ï¼Œä¾¿äºå¹¶è¡Œ / GPU è¿ç§»
  * æ¯ä¸ª sweep å¤§çº¦æ¶ˆè€— `L*L` ä¸ª uniform RNG è°ƒç”¨
* Wolff / Swendsenâ€“Wangï¼š

  * ä½¿ç”¨ Unionâ€“Findï¼ˆDSUï¼‰ç®¡ç†ç°‡
  * meta å­—æ®µä¸­ä¼šè®°å½• `cluster_size`ã€`num_clusters` ç­‰ä¿¡æ¯ï¼ˆè§†å®ç°è€Œå®šï¼‰

### `core.gpu_algorithms`

* ä¾èµ– CuPyï¼Œç»ä¼˜åŒ–é€‚é… REMC ä½¿ç”¨åœºæ™¯ï¼š

  * `metropolis_update_batch(spins, beta_list, ...)` ä¸€æ¬¡æ›´æ–°æ‰€æœ‰æ¸©åº¦æ§½ä¸Šçš„æ‰€æœ‰å‰¯æœ¬
  * å¯é€‰ `device_counters` / `replica_counters` å‚æ•°ï¼Œç”¨äºè®°å½• RNG æ¶ˆè€—
* èƒ½é‡ä¸ç£åŒ–ï¼š

  * `device_energy(spins, h)`
  * ï¼ˆå¯é€‰ï¼‰`device_magnetization(spins)`

### `core.observables`

* åœ¨ CPU ä¸Šè®¡ç®—å•ä¸ªæˆ–ä¸€æ‰¹ lattice çš„ç‰©ç†é‡ï¼š

  * `_observables_for_simulator(latt, h)` â†’ `{"E", "M", "absM", "M2", "M4"}`
* REMC ä¸­çš„ CPU / GPU ç‰ˆæœ¬éƒ½ä½¿ç”¨ **ç»Ÿä¸€çš„èƒ½é‡å®šä¹‰**ï¼š

  * å››é‚»å±…é…å¯¹ + 1/2 å› å­ï¼ˆé¿å…é‡å¤è®¡æ•°ï¼‰
  * åŠ ä¸Šå¤–åœºé¡¹ï¼š`- h * Î£_i s_i`

### `simulation.remc_simulator.HybridREMCSimulator`

* ä¸»è¦ç‰¹æ€§ï¼š

  * Slot-bound RNGï¼šæ¯ä¸ªæ¸©åº¦æ§½ä¸€ä¸ª `np.random.Generator`
  * åˆå§‹åŒ–ç”¨ `seed ^ 0xC2B2AE35` çš„ç‹¬ç«‹ RNG ç”Ÿæˆåˆå§‹æ„å‹ï¼Œä¿è¯åˆå§‹åŒ–ä¸åç»­æ¼”åŒ–çš„éšæœºæµè§£è€¦
  * æ”¯æŒ Metropolis / ç°‡ç®—æ³•ï¼ˆWolff / SWï¼‰
  * è‡ªé€‚åº” thinï¼ˆå¯é€‰ï¼‰ï¼šæ ¹æ®åœ¨çº¿ä¼°è®¡çš„è‡ªç›¸å…³æ—¶é—´è‡ªåŠ¨è°ƒèŠ‚é‡‡æ ·é—´éš”
* é‡è¦æ–¹æ³•ï¼š

  * `run(...)`ï¼šå®Œæˆå¹³è¡¡ + é‡‡æ · + ï¼ˆå¯é€‰ï¼‰æ ¼ç‚¹ä¿å­˜
  * `analyze(...)`ï¼šè¿”å›æ¯ä¸ªæ¸©åº¦ä¸‹çš„ `C`, `chi`, `U`, `n_samples` ç­‰
  * `save_checkpoint(...)` / `restore_from_checkpoint(...)`ï¼šæ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„æ–­ç‚¹ç»­ç®—

### `simulation.gpu_remc_simulator.GPU_REMC_Simulator`

* ä¸ CPU ç‰ˆä¿æŒè¯­ä¹‰ä¸€è‡´çš„ GPU ç‰ˆæœ¬ï¼š

  * æ‰€æœ‰å‰¯æœ¬è‡ªæ—‹æ„å‹é©»ç•™åœ¨ GPU ä¸Š
  * RE Metropolis æ›´æ–°åœ¨ GPU ä¸Šå‘é‡åŒ–å®Œæˆ
  * æ¸©åº¦äº¤æ¢ï¼ˆswapï¼‰åœ¨ CPU ä¸Šå®Œæˆï¼Œä½¿ç”¨ host RNG è¿›è¡Œæ¥å—åˆ¤æ®
* è¾“å‡ºæ¥å£ä¸ CPU ç‰ˆå°½é‡ç»Ÿä¸€ï¼š

  * `run(...)` / `analyze(...)`
  * æä¾›æœ€ç»ˆ lattice åˆ—è¡¨ `final_lattices`ï¼ˆåœ¨ host ä¸Šï¼‰

### `analysis.statistics`

* è‡ªç›¸å…³æ—¶é—´ä¸è¯¯å·®ä¼°è®¡å·¥å…·ï¼š

  * `estimate_block_len(series)`
  * `moving_block_bootstrap_error(series, func, ...)`
* REMC çš„ `analyze()` ä¼šè°ƒç”¨æ­¤æ¨¡å—ï¼Œå¯¹æ¯”çƒ­ / ç£åŒ–ç‡ç»™å‡º bootstrap è¯¯å·®ä¼°è®¡ã€‚


---

## ğŸ“š æ›´å¤šç¤ºä¾‹

ä»“åº“çš„ `examples/` ç›®å½•å»ºè®®åŒ…å«


---

## ğŸ“– å¼•ç”¨

å¦‚æœæœ¬å·¥å…·åŒ…å¯¹æ‚¨çš„ç ”ç©¶æˆ–æ•™å­¦æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{ising_fss,
  title  = {Ising-FSS: A High-Performance Toolkit for Finite-Size Scaling Analysis},
  author = {Li},
  year   = {2025},
  url    = {https://github.com/liyongxin0123/Ising-FSS}
}
```

**ç›¸å…³ç‰©ç†æ–‡çŒ®ï¼š**

* L. Onsager, *Phys. Rev.* **65**, 117 (1944) â€“ 2D Ising æ¨¡å‹ç²¾ç¡®è§£
* R. H. Swendsen, J.-S. Wang, *Phys. Rev. Lett.* **58**, 86 (1987) â€“ Swendsenâ€“Wang ç°‡ç®—æ³•
* U. Wolff, *Phys. Rev. Lett.* **62**, 361 (1989) â€“ Wolff å•ç°‡ç®—æ³•
* A. M. Ferrenberg, R. H. Swendsen, *Phys. Rev. Lett.* **61**, 2635 (1988) â€“ é‡åŠ æƒä¸ FSS åˆ†æ

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MIT License](LICENSE) å¼€æºï¼Œæ¬¢è¿åœ¨ç¬¦åˆåè®®çš„æ¡ä»¶ä¸‹è‡ªç”±ä½¿ç”¨ä¸ä¿®æ”¹ã€‚

---

## ğŸ™ è‡´è°¢

* **NumPy / SciPy / Numba** ç­‰ç§‘å­¦è®¡ç®—ç”Ÿæ€
* **CuPy** å›¢é˜Ÿï¼ˆæä¾›æ˜“ç”¨çš„ GPU æ•°ç»„è®¡ç®—æ¥å£ï¼‰
* **h5py** ä¸ HDF5 ç”Ÿæ€ï¼ˆé«˜æ€§èƒ½æ•°æ®å­˜å‚¨ï¼‰
* æ‰€æœ‰åœ¨ Ising æ¨¡å‹ä¸ FSS ç†è®ºæ–¹é¢åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…

---

## ğŸ“§ è”ç³»æ–¹å¼

* Issue: GitHub Issuesï¼ˆä¾‹å¦‚ï¼š`https://github.com/liyongxin0123/Ising-FSS/issues`ï¼‰


---





